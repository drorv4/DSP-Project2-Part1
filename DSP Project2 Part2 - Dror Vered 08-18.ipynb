{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2 - Shelter Animal Outcomes\n",
    "\n",
    "\n",
    "## Submitted by: Dror Vered\n",
    "\n",
    "### August 2018\n",
    "\n",
    "The goal of this project is to predict the outcome of the dogs and the cats as they leave the Austin Animal Center.\n",
    "These outcomes include: Adoption, Died, Euthanasia, Return to owner, and Transfer.\n",
    "\n",
    "### Part 2:\n",
    "Run machine learning models to predict the required outcome\n",
    "\n",
    "-----------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My Work Plan:\n",
    "\n",
    "- Apply data-related and model-related pre-processing steps as necessary (several rounds)\n",
    "- Apply several relevant models\n",
    "- Present final (best) results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 759,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn import neighbors, linear_model\n",
    "from sklearn.model_selection import train_test_split as split\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.preprocessing import StandardScaler, MaxAbsScaler, PolynomialFeatures\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import VarianceThreshold, SelectKBest\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import VotingClassifier, BaggingClassifier, \\\n",
    "                             AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the outcomes DF, as saved in Part1 of the project\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('part1outcomes', 'rb') as f:\n",
    "    outcomes = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outcomes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outcomes.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Pre-processing steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Feature Selection (round I)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen in Part 1 of the project, some of the features in the given data set are not relevant for the predictive models. \n",
    "These features are: \n",
    "- **Name:** The pet's name is not relevant. However, the 'Named' feature that I've added to the df will be in use\n",
    "- **DateTime:** As this is the *outcome* date, this feature cannot help us with predicting the outcome type\n",
    "- **OutcomeSubtype:** In part 1 of the project I showed some interesting findings regarding this feature, however it is not relevant for the predictive models\n",
    "- **SexuponOutcome:** Instead of this feature, I've created two separate features (Sex and NeuterStatus) which will be in use here\n",
    "- **AgeuponOutcome:** Instead of this feature, I've created the LifeStage feature, which I'll use here\n",
    "- **Breed:** Instead of this feature, I've created several other features which I'll use here (BreedPurity, MainBreed, DogFamily) \n",
    "- **Color:** Instead of this feature, I've created two separate features (ColorCoat and ColorGroup) which will be in use here\n",
    "\n",
    "In addition, some of the features that I've added in part 1, while investigating the data, are not relevant for this part of the project. These feature are: **DayOfWeek**, **Quarter** and **Holiday**.\n",
    "\n",
    "**The features mentioned above will be dropped from the data set that will serve the predictive models.**\n",
    "\n",
    "Note: Although I couldn't identify specific insights regarding 'lowBarking' and 'toleratesCold' features,  I choose not to drop them for now.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 737,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes2 = outcomes.drop(columns=\n",
    "       ['Name','DateTime','OutcomeSubtype','SexuponOutcome','AgeuponOutcome','Breed','Color','DayOfWeek','Quarter','Holiday'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outcomes2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outcomes2.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 738,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = outcomes2.drop('OutcomeType', axis=1)\n",
    "y = outcomes2.OutcomeType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 739,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.get_dummies(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The given *test.csv* file is not useful, as it doesn't contains the OutcomeType feature. Therefore I will split the outcome DF to train/test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 740,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = split(X, y, train_size=0.7, random_state=4014)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Naive classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Decision Tree model (as a start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 745,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=20,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=100, min_samples_split=100,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 745,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DT_clf = DecisionTreeClassifier(max_depth=20, min_samples_split=100, min_samples_leaf=100)\n",
    "DT_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 746,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.6526180670695834\n",
      "Test:  0.6396306463688545\n"
     ]
    }
   ],
   "source": [
    "print('Train:', DT_clf.score(X_train, y_train))\n",
    "print('Test: ', DT_clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting the Tree\n",
    "export_graphviz(decision_tree=DT_clf, out_file='outcomes_tree.dot', feature_names=X.columns, class_names=DT_clf.classes_,\n",
    "                leaves_parallel=True, filled=True, rotate=False, rounded=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1. Adding data scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6526180670695834\n",
      "0.6396306463688545\n"
     ]
    }
   ],
   "source": [
    "norm_scaler = StandardScaler()\n",
    "X_train_scaled = norm_scaler.fit_transform(X_train)\n",
    "\n",
    "DT_clf.fit(X_train_scaled, y_train)\n",
    "print(DT_clf.score(X_train_scaled, y_train))\n",
    "\n",
    "X_test_scaled = norm_scaler.transform(X_test)\n",
    "print(DT_clf.score(X_test_scaled, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2. With PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6350216612290742\n",
      "0.6071874220114799\n"
     ]
    }
   ],
   "source": [
    "pca_transformer = PCA(20)\n",
    "X_train_scaled_pca = pca_transformer.fit_transform(X_train_scaled)\n",
    "                                                   \n",
    "DT_clf.fit(X_train_scaled_pca, y_train)\n",
    "print(DT_clf.score(X_train_scaled_pca, y_train))\n",
    "                                                   \n",
    "X_test_scaled_pca = pca_transformer.transform(X_test_scaled)\n",
    "print(DT_clf.score(X_test_scaled_pca, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Separating Dogs/Cats (investigation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check if the features that I've created and added in Part1 of the project are useful (at all), I will now try to apply them separately on Dogs and Cats. If the results are significantly better than above, then I'll try to improve the complete data set using more pre-processing steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dogs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog_columns = [col for col in list(outcomes2.columns) if 'Dog' in col]\n",
    "\n",
    "outcomes2_Dogs = outcomes2[outcomes2.AnimalType == 'Dog']\n",
    "outcomes2_Dogs.drop(columns='AnimalType', inplace=True)    # there's no use of the AnimalType feature (all pets are dogs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.5834173156207054\n",
      "Test:  0.5753366103868348\n"
     ]
    }
   ],
   "source": [
    "X_Dogs = outcomes2_Dogs.drop('OutcomeType', axis=1)\n",
    "y_Dogs = outcomes2_Dogs.OutcomeType\n",
    "\n",
    "X_Dogs = pd.get_dummies(X_Dogs)\n",
    "\n",
    "X_Dogs_train, X_Dogs_test, y_Dogs_train, y_Dogs_test = split(X_Dogs, y_Dogs, train_size=0.7, random_state=4014)\n",
    "\n",
    "DT_clf = DecisionTreeClassifier(max_depth=15, min_samples_split=70, min_samples_leaf=70)\n",
    "DT_clf.fit(X_Dogs_train, y_Dogs_train)\n",
    "\n",
    "print('Train:', DT_clf.score(X_Dogs_train, y_Dogs_train))\n",
    "print('Test: ', DT_clf.score(X_Dogs_test, y_Dogs_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the external features I've added in Part1 include lots of missing data (marked as 999), I will now try to \"play\" with it in several alternative ways:\n",
    "- Filter out the 999 samples\n",
    "- Fill missing data with median value of the relevant column's values\n",
    "- Categorize the external features\n",
    "- Drop the external features\n",
    "\n",
    "In addition, I will check the Score when dropping the MainBreed feature, which is very varied (relying on the DogFamily, BreedPurity and other Dog features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outcomes2_Dogs.DogSize.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.5988733042078639\n",
      "Test:  0.5733440600697238\n"
     ]
    }
   ],
   "source": [
    "# Filter out the 999 samples\n",
    "\n",
    "outcomes2_Dogs = outcomes2[(outcomes2.AnimalType == 'Dog') & (outcomes2.DogSize != 999)]\n",
    "outcomes2_Dogs.drop(columns='AnimalType', inplace=True)\n",
    "\n",
    "X_Dogs = outcomes2_Dogs.drop('OutcomeType', axis=1)\n",
    "y_Dogs = outcomes2_Dogs.OutcomeType\n",
    "\n",
    "X_Dogs = pd.get_dummies(X_Dogs)\n",
    "\n",
    "X_Dogs_train, X_Dogs_test, y_Dogs_train, y_Dogs_test = split(X_Dogs, y_Dogs, train_size=0.7, random_state=4014)\n",
    "\n",
    "DT_clf = DecisionTreeClassifier(max_depth=15, min_samples_split=70, min_samples_leaf=70)\n",
    "DT_clf.fit(X_Dogs_train, y_Dogs_train)\n",
    "\n",
    "print('Train:', DT_clf.score(X_Dogs_train, y_Dogs_train))\n",
    "print('Test: ', DT_clf.score(X_Dogs_test, y_Dogs_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.5830508474576271\n",
      "Test:  0.5736268433425946\n"
     ]
    }
   ],
   "source": [
    "# Fill missing data with median value of the relevant column's values\n",
    "\n",
    "outcomes2_Dogs = outcomes2[outcomes2.AnimalType == 'Dog']\n",
    "outcomes2_Dogs.drop(columns='AnimalType', inplace=True)\n",
    "\n",
    "for col in dog_columns:   \n",
    "    if outcomes2_Dogs[col].dtype == 'int32':\n",
    "        outcomes2_Dogs[col] = outcomes2_Dogs[col].where(outcomes2_Dogs[col] < 999)   # convert 999 to NaN\n",
    "        outcomes2_Dogs[col].fillna(outcomes2_Dogs[col].median(), inplace=True)       # set median value where NaN\n",
    "\n",
    "X_Dogs = outcomes2_Dogs.drop('OutcomeType', axis=1)\n",
    "y_Dogs = outcomes2_Dogs.OutcomeType\n",
    "\n",
    "X_Dogs = pd.get_dummies(X_Dogs)\n",
    "\n",
    "X_Dogs_train, X_Dogs_test, y_Dogs_train, y_Dogs_test = split(X_Dogs, y_Dogs, train_size=0.7, random_state=4014)\n",
    "\n",
    "DT_clf = DecisionTreeClassifier(max_depth=15, min_samples_split=70, min_samples_leaf=70)\n",
    "DT_clf.fit(X_Dogs_train, y_Dogs_train)\n",
    "\n",
    "print('Train:', DT_clf.score(X_Dogs_train, y_Dogs_train))\n",
    "print('Test: ', DT_clf.score(X_Dogs_test, y_Dogs_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.5759047182775996\n",
      "Test:  0.5697798674930541\n"
     ]
    }
   ],
   "source": [
    "# Categorize the Dogs features\n",
    "\n",
    "outcomes2_Dogs = outcomes2[outcomes2.AnimalType == 'Dog']\n",
    "outcomes2_Dogs.drop(columns='AnimalType', inplace=True)\n",
    "\n",
    "X_Dogs = outcomes2_Dogs.drop('OutcomeType', axis=1)\n",
    "y_Dogs = outcomes2_Dogs.OutcomeType\n",
    "\n",
    "cols = [col for col in list(X_Dogs.columns) if col != 'Named']\n",
    "\n",
    "X_Dogs = pd.get_dummies(X_Dogs, columns=cols)\n",
    "\n",
    "X_Dogs_train, X_Dogs_test, y_Dogs_train, y_Dogs_test = split(X_Dogs, y_Dogs, train_size=0.7, random_state=4014)\n",
    "\n",
    "DT_clf = DecisionTreeClassifier(max_depth=10, min_samples_split=100, min_samples_leaf=100)\n",
    "DT_clf.fit(X_Dogs_train, y_Dogs_train)\n",
    "\n",
    "print('Train:', DT_clf.score(X_Dogs_train, y_Dogs_train))\n",
    "print('Test: ', DT_clf.score(X_Dogs_test, y_Dogs_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.574072377462208\n",
      "Test:  0.5680701004488139\n"
     ]
    }
   ],
   "source": [
    "# Drop all external Dogs features\n",
    "\n",
    "dog_columns = [col for col in list(outcomes2.columns) if ('Dog' in col and col != 'DogFamily')]\n",
    "\n",
    "outcomes2_Dogs = outcomes2[outcomes2.AnimalType == 'Dog']\n",
    "outcomes2_Dogs.drop(columns='AnimalType', inplace=True)\n",
    "\n",
    "outcomes2_Dogs.drop(columns=dog_columns, inplace=True)\n",
    "\n",
    "X_Dogs = outcomes2_Dogs.drop('OutcomeType', axis=1)\n",
    "y_Dogs = outcomes2_Dogs.OutcomeType\n",
    "\n",
    "X_Dogs = pd.get_dummies(X_Dogs)\n",
    "\n",
    "X_Dogs_train, X_Dogs_test, y_Dogs_train, y_Dogs_test = split(X_Dogs, y_Dogs, train_size=0.7, random_state=4014)\n",
    "\n",
    "DT_clf = DecisionTreeClassifier(max_depth=10, min_samples_split=100, min_samples_leaf=100)\n",
    "DT_clf.fit(X_Dogs_train, y_Dogs_train)\n",
    "\n",
    "print('Train:', DT_clf.score(X_Dogs_train, y_Dogs_train))\n",
    "print('Test: ', DT_clf.score(X_Dogs_test, y_Dogs_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.5770041227668347\n",
      "Test:  0.5734131224620646\n"
     ]
    }
   ],
   "source": [
    "# Drop most of external Dogs features (all but DogSize)\n",
    "\n",
    "dog_columns = [col for col in list(outcomes2.columns) if ('Dog' in col and col not in ['DogFamily','DogSize'])]\n",
    "\n",
    "outcomes2_Dogs = outcomes2[outcomes2.AnimalType == 'Dog']\n",
    "outcomes2_Dogs.drop(columns='AnimalType', inplace=True)\n",
    "\n",
    "outcomes2_Dogs.drop(columns=dog_columns, inplace=True)\n",
    "\n",
    "X_Dogs = outcomes2_Dogs.drop('OutcomeType', axis=1)\n",
    "y_Dogs = outcomes2_Dogs.OutcomeType\n",
    "\n",
    "X_Dogs = pd.get_dummies(X_Dogs)\n",
    "\n",
    "X_Dogs_train, X_Dogs_test, y_Dogs_train, y_Dogs_test = split(X_Dogs, y_Dogs, train_size=0.7, random_state=4014)\n",
    "\n",
    "DT_clf = DecisionTreeClassifier(max_depth=10, min_samples_split=100, min_samples_leaf=100)\n",
    "DT_clf.fit(X_Dogs_train, y_Dogs_train)\n",
    "\n",
    "print('Train:', DT_clf.score(X_Dogs_train, y_Dogs_train))\n",
    "print('Test: ', DT_clf.score(X_Dogs_test, y_Dogs_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.5757214841960605\n",
      "Test:  0.5734131224620646\n"
     ]
    }
   ],
   "source": [
    "# Drop the MainBreed feature\n",
    "\n",
    "outcomes2_Dogs = outcomes2[outcomes2.AnimalType == 'Dog']\n",
    "outcomes2_Dogs.drop(columns='AnimalType', inplace=True)\n",
    "\n",
    "X_Dogs = outcomes2_Dogs.drop(columns=['OutcomeType', 'MainBreed'])\n",
    "y_Dogs = outcomes2_Dogs.OutcomeType\n",
    "\n",
    "X_Dogs = pd.get_dummies(X_Dogs)\n",
    "\n",
    "X_Dogs_train, X_Dogs_test, y_Dogs_train, y_Dogs_test = split(X_Dogs, y_Dogs, train_size=0.7, random_state=4014)\n",
    "\n",
    "DT_clf = DecisionTreeClassifier(max_depth=10, min_samples_split=100, min_samples_leaf=100)\n",
    "DT_clf.fit(X_Dogs_train, y_Dogs_train)\n",
    "\n",
    "print('Train:', DT_clf.score(X_Dogs_train, y_Dogs_train))\n",
    "print('Test: ', DT_clf.score(X_Dogs_test, y_Dogs_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that all the above manipulations give approx the same results, all of them are worse than the \"naive\" model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cats**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes2_Cats = outcomes2[outcomes2.AnimalType == 'Cat']\n",
    "outcomes2_Cats.drop(columns='AnimalType', inplace=True)    # there's no use of the AnimalType feature (all pets are cats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I'll drop all the features that are relevant only for dogs  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog_columns = [col for col in list(outcomes2.columns) if 'Dog' in col]\n",
    "\n",
    "outcomes2_Cats.drop(columns=dog_columns, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 750,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.7572291479244313\n",
      "Test:  0.7428057553956835\n"
     ]
    }
   ],
   "source": [
    "X_Cats = outcomes2_Cats.drop('OutcomeType', axis=1)\n",
    "\n",
    "y_Cats = outcomes2_Cats.OutcomeType\n",
    "\n",
    "X_Cats = pd.get_dummies(X_Cats)\n",
    "\n",
    "X_Cats_train, X_Cats_test, y_Cats_train, y_Cats_test = split(X_Cats, y_Cats, train_size=0.7, random_state=4014)\n",
    "\n",
    "DT_clf = DecisionTreeClassifier(max_depth=15, min_samples_split=50, min_samples_leaf=50)\n",
    "DT_clf.fit(X_Cats_train, y_Cats_train)\n",
    "\n",
    "print('Train:', DT_clf.score(X_Cats_train, y_Cats_train))\n",
    "print('Test: ', DT_clf.score(X_Cats_test, y_Cats_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better result for the Cats portion of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Selection (round II)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check features' importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuterStatus_N/S                                0.553925\n",
       "LifeStage_Puppy/Kitten                          0.185372\n",
       "Named                                           0.078532\n",
       "AnimalType_Cat                                  0.042392\n",
       "LifeStage_Junior                                0.039681\n",
       "AnimalType_Dog                                  0.024333\n",
       "LifeStage_Neonatal                              0.013899\n",
       "DogSize                                         0.010680\n",
       "DogLowShedding                                  0.004496\n",
       "LifeStage_Senior                                0.004349\n",
       "DogToleratesCold                                0.004190\n",
       "LifeStage_Prime                                 0.004099\n",
       "Sex_Male                                        0.003848\n",
       "Sex_Female                                      0.003253\n",
       "BreedPurity_Crossbreeds                         0.002971\n",
       "DogEasyToGroom                                  0.002781\n",
       "DogFamily_Pit Bull                              0.002519\n",
       "DogHighEnergy                                   0.002297\n",
       "DogFamily_Irrelevant                            0.002268\n",
       "ColorCoat_Bicolor                               0.001757\n",
       "ColorCoat_Solid                                 0.001561\n",
       "DogIntelligence                                 0.001202\n",
       "ColorCoat_Tabby                                 0.001091\n",
       "DogLowBarking                                   0.000900\n",
       "LifeStage_Mature                                0.000838\n",
       "MainBreed_Pit Bull                              0.000790\n",
       "DogGoodHealth                                   0.000692\n",
       "ColorGroup_Brown/White                          0.000674\n",
       "ColorGroup_Black/White                          0.000624\n",
       "DogFamily_Herding                               0.000586\n",
       "                                                  ...   \n",
       "MainBreed_Rottweiler                            0.000000\n",
       "MainBreed_Rhod Ridgeback                        0.000000\n",
       "MainBreed_Redbone Hound                         0.000000\n",
       "MainBreed_Rat Terrier                           0.000000\n",
       "MainBreed_Ragdoll                               0.000000\n",
       "MainBreed_Queensland Heeler                     0.000000\n",
       "MainBreed_Pug                                   0.000000\n",
       "MainBreed_Presa Canario                         0.000000\n",
       "MainBreed_Port Water Dog                        0.000000\n",
       "MainBreed_Pomeranian                            0.000000\n",
       "MainBreed_Pointer                               0.000000\n",
       "MainBreed_Podengo Pequeno                       0.000000\n",
       "MainBreed_Plott Hound                           0.000000\n",
       "MainBreed_Picardy Sheepdog                      0.000000\n",
       "MainBreed_Norwegian Forest Cat                  0.000000\n",
       "MainBreed_Pharaoh Hound                         0.000000\n",
       "MainBreed_Persian                               0.000000\n",
       "MainBreed_Pembroke Welsh Corgi                  0.000000\n",
       "MainBreed_Pekingese                             0.000000\n",
       "MainBreed_Pbgv                                  0.000000\n",
       "MainBreed_Patterdale Terr                       0.000000\n",
       "MainBreed_Parson Russell Terrier                0.000000\n",
       "MainBreed_Papillon                              0.000000\n",
       "MainBreed_Otterhound                            0.000000\n",
       "MainBreed_Old English Sheepdog                  0.000000\n",
       "MainBreed_Old English Bulldog                   0.000000\n",
       "MainBreed_Ocicat                                0.000000\n",
       "MainBreed_Nova Scotia Duck Tolling Retriever    0.000000\n",
       "MainBreed_Norwich Terrier                       0.000000\n",
       "MainBreed_Italian Greyhound                     0.000000\n",
       "Length: 295, dtype: float64"
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = outcomes2.drop('OutcomeType', axis=1)\n",
    "y = outcomes2.OutcomeType\n",
    "X = pd.get_dummies(X)\n",
    "X_train, X_test, y_train, y_test = split(X, y, train_size=0.7, random_state=4014)\n",
    "DT_clf = DecisionTreeClassifier(max_depth=20, min_samples_split=100, min_samples_leaf=100)\n",
    "DT_clf.fit(X_train, y_train)\n",
    "\n",
    "pd.Series(DT_clf.feature_importances_, index=X.columns).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above finding, I'll try to run the model with the following features solely:\n",
    "\n",
    "NeuterStatus, LifeStage, Named, AnimalType, DogSize, DogLowShedding, DogToleratesCold, Sex, BreedPurity, DogEasyToGroom, DogFamily\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.6512809541637696\n",
      "Test:  0.6421262790117295\n"
     ]
    }
   ],
   "source": [
    "not_important_cols = [col for col in list(outcomes2.columns) if col not in\n",
    "                     ['NeuterStatus','LifeStage','Named','AnimalType','DogSize','DogLowShedding',\n",
    "                      'DogToleratesCold','Sex','BreedPurity','DogEasyToGroom','DogFamily']]\n",
    "X_important = outcomes2.drop(columns=not_important_cols)\n",
    "\n",
    "X_important = pd.get_dummies(X_important)\n",
    "\n",
    "X_important_train, X_important_test, y_train, y_test = split(X_important, y, train_size=0.7, random_state=4014)\n",
    "\n",
    "DT_clf = DecisionTreeClassifier(max_depth=10, min_samples_split=50, min_samples_leaf=50)\n",
    "DT_clf.fit(X_important_train, y_train)\n",
    "\n",
    "print('Train:', DT_clf.score(X_important_train, y_train))\n",
    "print('Test: ', DT_clf.score(X_important_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A slight improvement (**best score so far...**)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. K-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=20, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 510,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = outcomes2.drop('OutcomeType', axis=1)\n",
    "y = outcomes2.OutcomeType\n",
    "X = pd.get_dummies(X)\n",
    "X_train, X_test, y_train, y_test = split(X, y, train_size=0.7, random_state=4014)\n",
    "\n",
    "KNN_clf = KNeighborsClassifier(n_neighbors=20) # I tried several k's and this seems to be the best\n",
    "                                              # I also examined 'euclidean' and 'manhattan' metrics,but there was no improvement\n",
    "KNN_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.6401561747873991\n",
      "Test:  0.6091839281257799\n"
     ]
    }
   ],
   "source": [
    "print('Train:', KNN_clf.score(X_train, y_train))\n",
    "print('Test: ', KNN_clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying to improve the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.6421351018880034\n",
      "Test:  0.6069378587471924\n"
     ]
    }
   ],
   "source": [
    "# Convert 999 values to median (as done above with the dogs)\n",
    "\n",
    "outcomes2_KNN = outcomes2.copy()\n",
    "\n",
    "for col in dog_columns:   \n",
    "    if outcomes2_KNN[col].dtype == 'int32':\n",
    "        outcomes2_KNN[col] = outcomes2_KNN[col].where(outcomes2_KNN[col] < 999)   # convert 999 to NaN\n",
    "        outcomes2_KNN[col].fillna(outcomes2_KNN[col].median(), inplace=True)      # set median value where NaN\n",
    "\n",
    "X = outcomes2_KNN.drop('OutcomeType', axis=1)\n",
    "y = outcomes2_KNN.OutcomeType\n",
    "X = pd.get_dummies(X)\n",
    "X_train, X_test, y_train, y_test = split(X, y, train_size=0.7, random_state=4014)\n",
    "\n",
    "KNN_clf = KNeighborsClassifier(n_neighbors=20) \n",
    "KNN_clf.fit(X_train, y_train)\n",
    "\n",
    "print('Train:', KNN_clf.score(X_train, y_train))\n",
    "print('Test: ', KNN_clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.6397282986575387\n",
      "Test:  0.6106813077115049\n"
     ]
    }
   ],
   "source": [
    "# Categorize the Dogs features\n",
    "\n",
    "outcomes2_KNN = outcomes2.copy()\n",
    "\n",
    "X = outcomes2_KNN.drop('OutcomeType', axis=1)\n",
    "y = outcomes2_KNN.OutcomeType\n",
    "cols = [col for col in list(X.columns) if col != 'Named']\n",
    "X = pd.get_dummies(X, columns=cols)\n",
    "X_train, X_test, y_train, y_test = split(X, y, train_size=0.7, random_state=4014)\n",
    "\n",
    "KNN_clf = KNeighborsClassifier(n_neighbors=20) \n",
    "KNN_clf.fit(X_train, y_train)\n",
    "\n",
    "print('Train:', KNN_clf.score(X_train, y_train))\n",
    "print('Test: ', KNN_clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A slight improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.6462534096379098\n",
      "Test:  0.620539056650861\n"
     ]
    }
   ],
   "source": [
    "# Using only the \"important features\" (as found above), with categorization\n",
    "\n",
    "outcomes2_KNN = outcomes2.copy()\n",
    "\n",
    "not_important_cols = [col for col in list(outcomes2_KNN.columns) if col not in\n",
    "                     ['NeuterStatus','LifeStage','Named','AnimalType','DogSize','DogLowShedding',\n",
    "                      'DogToleratesCold','Sex','BreedPurity','DogEasyToGroom','DogFamily']]\n",
    "X_important = outcomes2_KNN.drop(columns=not_important_cols)\n",
    "\n",
    "cols = [col for col in list(X_important.columns) if col != 'Named']\n",
    "X_important = pd.get_dummies(X_important, columns=cols)\n",
    "\n",
    "X_important_train, X_important_test, y_train, y_test = split(X_important, y, train_size=0.7, random_state=4014)\n",
    "\n",
    "KNN_clf = KNeighborsClassifier(n_neighbors=20) \n",
    "KNN_clf.fit(X_important_train, y_train)\n",
    "\n",
    "print('Train:', KNN_clf.score(X_important_train, y_train))\n",
    "print('Test: ', KNN_clf.score(X_important_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.6488206664170723\n",
      "Test:  0.636386323933117\n"
     ]
    }
   ],
   "source": [
    "X = outcomes2.drop('OutcomeType', axis=1)\n",
    "y = outcomes2.OutcomeType\n",
    "X = pd.get_dummies(X)\n",
    "X_train, X_test, y_train, y_test = split(X, y, train_size=0.7, random_state=4014)\n",
    "\n",
    "LR_clf = LogisticRegression().fit(X_train, y_train)\n",
    "print('Train:', LR_clf.score(X_train, y_train))\n",
    "print('Test: ', LR_clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying to improve the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.6514414077124673\n",
      "Test:  0.6348889443473921\n"
     ]
    }
   ],
   "source": [
    "# Categorize the Dogs features\n",
    "\n",
    "outcomes2_LR = outcomes2.copy()\n",
    "\n",
    "X = outcomes2_LR.drop('OutcomeType', axis=1)\n",
    "y = outcomes2_LR.OutcomeType\n",
    "cols = [col for col in list(X.columns) if col != 'Named']\n",
    "X = pd.get_dummies(X, columns=cols)\n",
    "X_train, X_test, y_train, y_test = split(X, y, train_size=0.7, random_state=4014)\n",
    "\n",
    "LR_clf = LogisticRegression().fit(X_train, y_train)\n",
    "\n",
    "print('Train:', LR_clf.score(X_train, y_train))\n",
    "print('Test: ', LR_clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.6458255335080494\n",
      "Test:  0.6395058647367108\n"
     ]
    }
   ],
   "source": [
    "# Using only the \"important features\" (as found above), with categorization\n",
    "\n",
    "outcomes2_LR = outcomes2.copy()\n",
    "\n",
    "not_important_cols = [col for col in list(outcomes2_LR.columns) if col not in\n",
    "                     ['NeuterStatus','LifeStage','Named','AnimalType','DogSize','DogLowShedding',\n",
    "                      'DogToleratesCold','Sex','BreedPurity','DogEasyToGroom','DogFamily']]\n",
    "X_important = outcomes2_LR.drop(columns=not_important_cols)\n",
    "\n",
    "cols = [col for col in list(X_important.columns) if col != 'Named']\n",
    "X_important = pd.get_dummies(X_important, columns=cols)\n",
    "\n",
    "X_important_train, X_important_test, y_train, y_test = split(X_important, y, train_size=0.7, random_state=4014)\n",
    "\n",
    "LR_clf = LogisticRegression().fit(X_important_train, y_train)\n",
    "\n",
    "print('Train:', LR_clf.score(X_important_train, y_train))\n",
    "print('Test: ', LR_clf.score(X_important_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A slight improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1. Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.5119537893779751\n",
      "Test:  0.50299475917145\n"
     ]
    }
   ],
   "source": [
    "X = outcomes2.drop('OutcomeType', axis=1)\n",
    "y = outcomes2.OutcomeType\n",
    "X = pd.get_dummies(X)\n",
    "X_train, X_test, y_train, y_test = split(X, y, train_size=0.7, random_state=4014)\n",
    "\n",
    "SVM_clf = LinearSVC().fit(X_train, y_train)\n",
    "print('Train:', SVM_clf.score(X_train, y_train))\n",
    "print('Test: ', SVM_clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bad result (not surprisingly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2. Non-linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.6332566721934\n",
      "Test:  0.6276516096830547\n"
     ]
    }
   ],
   "source": [
    "X = outcomes2.drop('OutcomeType', axis=1)\n",
    "y = outcomes2.OutcomeType\n",
    "X = pd.get_dummies(X)\n",
    "X_train, X_test, y_train, y_test = split(X, y, train_size=0.7, random_state=4014)\n",
    "\n",
    "SVM_clf = SVC(kernel='rbf').fit(X_train, y_train)\n",
    "print('Train:', SVM_clf.score(X_train, y_train))\n",
    "print('Test: ', SVM_clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying to improve the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.6403166283360967\n",
      "Test:  0.6376341402545546\n"
     ]
    }
   ],
   "source": [
    "# Categorize the Dogs features\n",
    "\n",
    "outcomes2_SVM = outcomes2.copy()\n",
    "\n",
    "X = outcomes2_SVM.drop('OutcomeType', axis=1)\n",
    "y = outcomes2_SVM.OutcomeType\n",
    "cols = [col for col in list(X.columns) if col != 'Named']\n",
    "X = pd.get_dummies(X, columns=cols)\n",
    "X_train, X_test, y_train, y_test = split(X, y, train_size=0.7, random_state=4014)\n",
    "\n",
    "SVM_clf = SVC(C=1, kernel='rbf').fit(X_train, y_train) # I tried several kernels and values of C. This seems to be the best\n",
    "\n",
    "print('Train:', SVM_clf.score(X_train, y_train))\n",
    "print('Test: ', SVM_clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A slight improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.6412793496282826\n",
      "Test:  0.6352632892438234\n"
     ]
    }
   ],
   "source": [
    "# Using only the \"important features\" (as found above), with categorization\n",
    "\n",
    "outcomes2_SVM = outcomes2.copy()\n",
    "\n",
    "not_important_cols = [col for col in list(outcomes2_SVM.columns) if col not in\n",
    "                     ['NeuterStatus','LifeStage','Named','AnimalType','DogSize','DogLowShedding',\n",
    "                      'DogToleratesCold','Sex','BreedPurity','DogEasyToGroom','DogFamily']]\n",
    "X_important = outcomes2_SVM.drop(columns=not_important_cols)\n",
    "\n",
    "cols = [col for col in list(X_important.columns) if col != 'Named']\n",
    "X_important = pd.get_dummies(X_important, columns=cols)\n",
    "\n",
    "X_important_train, X_important_test, y_train, y_test = split(X_important, y, train_size=0.7, random_state=4014)\n",
    "\n",
    "SVM_clf = SVC(C=3, kernel='rbf').fit(X_important_train, y_train)\n",
    "\n",
    "print('Train:', SVM_clf.score(X_important_train, y_train))\n",
    "print('Test: ', SVM_clf.score(X_important_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Selection (round III)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1. VarianceThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26711, 354)\n",
      "(18697, 33)\n"
     ]
    }
   ],
   "source": [
    "X = outcomes2.drop('OutcomeType', axis=1)\n",
    "y = outcomes2.OutcomeType\n",
    "cols = [col for col in list(X.columns) if col != 'Named']\n",
    "X = pd.get_dummies(X, columns=cols)\n",
    "print(X.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = split(X, y, train_size=0.7, random_state=4014)\n",
    "\n",
    "selector = VarianceThreshold(threshold=0.16)  # 0.8 * 0.2\n",
    "X_train_high_variance = selector.fit_transform(X_train)\n",
    "X_test_high_variance = selector.fit_transform(X_test)\n",
    "\n",
    "print(X_train_high_variance.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT:\n",
      "===\n",
      "Train: 0.6429908541477243\n",
      "Test:  0.6397554280009983\n"
     ]
    }
   ],
   "source": [
    "print('DT:\\n===')\n",
    "DT_clf = DecisionTreeClassifier(max_depth=20, min_samples_split=100, min_samples_leaf=100)\n",
    "DT_clf.fit(X_train_high_variance, y_train)\n",
    "print('Train:', DT_clf.score(X_train_high_variance, y_train))\n",
    "print('Test: ', DT_clf.score(X_test_high_variance, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-NN:\n",
      "=====\n",
      "Train: 0.6464138631866074\n",
      "Test:  0.6271524831544797\n"
     ]
    }
   ],
   "source": [
    "print('K-NN:\\n=====')\n",
    "KNN_clf = KNeighborsClassifier(n_neighbors=20)\n",
    "KNN_clf.fit(X_train_high_variance, y_train)\n",
    "print('Train:', KNN_clf.score(X_train_high_variance, y_train))\n",
    "print('Test: ', KNN_clf.score(X_test_high_variance, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR:\n",
      "===\n",
      "Train: 0.6383377012354923\n",
      "Test:  0.6350137259795358\n"
     ]
    }
   ],
   "source": [
    "print('LR:\\n===')\n",
    "LR_clf = LogisticRegression().fit(X_train_high_variance, y_train)\n",
    "print('Train:', LR_clf.score(X_train_high_variance, y_train))\n",
    "print('Test: ', LR_clf.score(X_test_high_variance, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM:\n",
      "====\n",
      "Train: 0.6346472696154464\n",
      "Test:  0.6322685300723734\n"
     ]
    }
   ],
   "source": [
    "print('SVM:\\n====')\n",
    "SVM_clf = SVC().fit(X_train_high_variance, y_train)\n",
    "print('Train:', SVM_clf.score(X_train_high_variance, y_train))\n",
    "print('Test: ', SVM_clf.score(X_test_high_variance, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2. SelectKBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26711, 354)\n",
      "(18697, 20)\n"
     ]
    }
   ],
   "source": [
    "X = outcomes2.drop('OutcomeType', axis=1)\n",
    "y = outcomes2.OutcomeType\n",
    "cols = [col for col in list(X.columns) if col != 'Named']\n",
    "X = pd.get_dummies(X, columns=cols)\n",
    "print(X.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = split(X, y, train_size=0.7, random_state=4014)\n",
    "\n",
    "X_train_KBest = SelectKBest(k=20).fit_transform(X_train, y_train)\n",
    "X_test_KBest = SelectKBest(k=20).fit_transform(X_test, y_test)\n",
    "\n",
    "print(X_train_KBest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT:\n",
      "===\n",
      "Train: 0.6187623682943788\n",
      "Test:  0.6140504117793861\n"
     ]
    }
   ],
   "source": [
    "print('DT:\\n===')\n",
    "DT_clf = DecisionTreeClassifier(max_depth=20, min_samples_split=30, min_samples_leaf=30)\n",
    "DT_clf.fit(X_train_KBest, y_train)\n",
    "print('Train:', DT_clf.score(X_train_KBest, y_train))\n",
    "print('Test: ', DT_clf.score(X_test_KBest, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-NN:\n",
      "=====\n",
      "Train: 0.6100978766647056\n",
      "Test:  0.6064387322186174\n"
     ]
    }
   ],
   "source": [
    "print('K-NN:\\n=====')\n",
    "KNN_clf = KNeighborsClassifier(n_neighbors=15)\n",
    "KNN_clf.fit(X_train_KBest, y_train)\n",
    "print('Train:', KNN_clf.score(X_train_KBest, y_train))\n",
    "print('Test: ', KNN_clf.score(X_test_KBest, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR:\n",
      "===\n",
      "Train: 0.6140557308659144\n",
      "Test:  0.6098078362864986\n"
     ]
    }
   ],
   "source": [
    "print('LR:\\n===')\n",
    "LR_clf = LogisticRegression().fit(X_train_KBest, y_train)\n",
    "print('Train:', LR_clf.score(X_train_KBest, y_train))\n",
    "print('Test: ', LR_clf.score(X_test_KBest, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM:\n",
      "====\n",
      "Train: 0.6113280205380542\n",
      "Test:  0.6081856750686299\n"
     ]
    }
   ],
   "source": [
    "print('SVM:\\n====')\n",
    "SVM_clf = SVC().fit(X_train_KBest, y_train)\n",
    "print('Train:', SVM_clf.score(X_train_KBest, y_train))\n",
    "print('Test: ', SVM_clf.score(X_test_KBest, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3. PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3.1. Finding number of principal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 720,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = outcomes2.drop('OutcomeType', axis=1)\n",
    "y = outcomes2.OutcomeType\n",
    "cols = [col for col in list(X.columns) if col != 'Named']\n",
    "X = pd.get_dummies(X, columns=cols)\n",
    "X_train, X_test, y_train, y_test = split(X, y, train_size=0.7, random_state=4014)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores = []\n",
    "test_scores = []\n",
    "for k in range(1, 30):\n",
    "    pca_transformer = PCA(n_components=k).fit(X_train)\n",
    "    X_train_pca = pca_transformer.transform(X_train)\n",
    "    clf = DecisionTreeClassifier(max_depth=20, min_samples_split=100, min_samples_leaf=100).fit(X_train_pca, y_train)\n",
    "    X_test_pca = pca_transformer.transform(X_test)\n",
    "    train_scores.append(clf.score(X_train_pca, y_train))\n",
    "    test_scores.append(clf.score(X_test_pca, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2648c322630>"
      ]
     },
     "execution_count": 704,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEXCAYAAABcRGizAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzs3Xl8FPX9+PHXO/cdAuEMRziCoKAgp6AiUhCwVdGqYL3aKmI9259+1W9br+q3tlartooVxVs8qqCtKKe3cioqd7gJZ0gg5L728/tjJrDZnU02YY8c7+fjsY/d/czMznsnk3nvfD6f+YwYY1BKKaXqEhHuAJRSSjV9miyUUkrVS5OFUkqpemmyUEopVS9NFkoppeqlyUIppVS9NFm0AiKSKSJGRKL8mPdaEfkyFHEpEJGXROShMK1bRORFETksIivCEYNqPjRZNDEiskNEKkQk3aN8jX3AzwxPZKoFOhMYD3Q1xgwPdzBNif5o8qbJomnaDkyreSMiA4H48IUTGv6c+SjfRCSygYv0AHYYY4qDEY9qWTRZNE2vAle7vb8GeMV9BhFJFZFXRCRXRHaKyB9EJMKeFikifxORQyKyDTjfYdkXRGSfiOwRkYf8OdCISJyIvCYieSJyRERWikhHe1pbu0pjr12tMc9tuetFZIuI5IvIByLSxW2aEZGbRCQbyLbL+onIInv+TSJymY94porIKo+y34rIB/brySKyXkQK7e95R33f0V7uJRF5WkQ+tJddLiK97WleVXoi8qmIXGe/vlZEvhKRv9vbaJuIjLLLd4vIQRG5xmOV6fb3LRSRz0Skh9tn+9wWdpwzRWS+iBQDYx2+Sxd7m+fbf4Pr7fJfA88DZ4hIkYg84GNbXC8iG+zY1ovI6XZ5f/t7HxGRdSJygUdcz4jIR/ZnfyUinUTkCXvf2Cgig93m3yEi99iff9jej+I8Yqhr/5khItn2sk+LiLhN/5Ud/2ERWeCxbR2XFZH+wLNu2+aIPX+j9qcWwxijjyb0AHYAPwE2Af2BSGA31q9AA2Ta870CvA8kA5nAZuDX9rQZwEagG9AW+MReNsqePg/4F5AIdABWADfY064FvvQR2w3Af4AEO64hQIo97UPgLSANiAbG2OXnAoeA04FY4B/A526faYBFdpzxdky7gV8CUfZyh4BTHOJJAAqBLLeylcBU+/U+4Cz7dRpwup9/g5eAfGC4HcPrwJv2tEz3bWmXfQpc57b9quz4I4GHgF3A0/b3n2DHnOS2rkLgbHv6kzXbv75tYS9bAIzG+uEX5/BdPgOeAeKAQUAuMK6+v7U9/VJgDzAMEKAP1n4YDWwB/heIsf/GhcBJbnEdsvePOGAp1tny1W7b5BOPfX4tx/fXr4CHGrD//BdoA3S3v99Ee9pFdpz97e33B+BrP5f12jY0cn9qKY+wB6APjz/I8WTxB+DPwESsg2mUvXNn2v9w5cDJbsvdAHxqv14KzHCbNsFeNgroaC8b7zZ9Ws0/b10HEOBXwNfAqR7lnQEXkOawzAvAX93eJwGVHE96BjjXbfrlwBcen/Ev4D4fMb0G3Gu/zrIPWgn2+132dklp4N/gJeB5t/eTgY3260zqTxbZbtMG2vN3dCvLAwa5retNj+1TjXXgrHNb2Mu+Usf36GZ/VrJb2Z+Bl+r7W9vTFwC3OZSfBewHItzK5gD3u8U1y23aLcAGj21yxGOfd99fJwNbG7D/nOk2/W3gbvv1R9g/oOz3EUAJ0MOPZb22TWP3p5by0GqoputV4AqsnfYVj2npWL/odrqV7QQy7NddsH6Ruk+rUfPLcJ9dhXAE6wDUwc+YFgBvilXd9FcRicY6KOUbYw47LNPFff3GmCKsg2WG2zzusfYARtTEZsf3C6CTj5je4Hj7zhXAPGNMif3+EqwDz067eucMP75jjf1ur0uwDlL+OuD2uhTAGONZ5v55x76/vX3ysbabP9vCfdt56oL1dyl0K3PfT+rTDdjq43N3G2NcdXyu5/et6/uD9/5aU9Xkz/7j62/VA3jSbdvlY50h+bOskxPZn5o9bVBsoowxO0VkO9bO+WuPyYewfl31ANbbZd2xqgzAOl3u5jZ/d7fXu7HOLNKNMVUNjKkSeAB4QKxeWfOxqsvmA21FpI0x5ojHYnvtOAEQkUSgnVusYP3Cc4/vM2PMeD/DWohV5z8IK2n81i3elcCFdkK7GeuXYzfHT/FfTWNwAnDUfu0rkfnrWEwikoRVFbMX/7ZFXcNG78X6uyS7JQz3/aQ+u4HePj63m4hEuCWM7lhVoY3lub/udVtXffuPL7uBh40xrzciHq/tGqT9qdnQM4um7ddYVTS1eqsYY6qxdtSHRSTZbrT7HVaVDPa0W0Wkq4ikAXe7LbsP6wD7mIikiEiEiPQWkTH1BSMiY0VkoFiN4UexEla1/ZkfAc+ISJqIRIvI2fZibwC/FJFBIhIL/B+w3Bizw8dq/gv0FZGr7M+JFpFhdqOjFzvh/Rt4FOsgu8iONUZEfiEiqXaSO4pVJXNCjDG5WAeqK8XqSPArnA+oDTFZRM4UkRjgT1jbZzcN3BYOse7Gqjb8s1idE07F2qf8PXg+D9whIkPsht8+9r62HCtp/o8d0znAz4A3G/KlPdxk769tsdpC3rLLG7r/uHsWuEdEToFjHTsu9TOeA0BX+28StP2pOdFk0YQZY7YaY1b5mHwL1j/sNuBLrH+q2fa0WVjVRd8D3wLveSx7NVY11nrgMNbBtrMfIXWy5z0KbMBqPK1JUFdhJY+NwEHgdvs7LAH+CLyLdcbTG5jqawX2L+AJ9jx7saoJ/oLVuOnLG1jtPO94nC1dBewQkaNYjf5XAohId7uXS3fvj/LL9cCdWNUhp2AdkE/EG8B9WNUkQ7Cqmhq7LTxNw2pn2QvMxWrvWOTPgsaYd4CH7fgKsTpGtDXGVAAXAJOwznKfAa42xmxsQFye3sD6EbPNfjxkx9Cg/ccj/rlY2+tNex9Ya8fsj6XAOmC/iByyyxz3p9ZC7IYbpZQKCxHZgdVBYHG4Y1G+6ZmFUkqpemmyUEopVS+thlJKKVUvPbNQSilVL00WSiml6tViLspLT083mZmZ4Q5DKaWaldWrVx8yxrSvb74WkywyMzNZtcrXJQlKKaWciMjO+ufSaiillFJ+0GShlFKqXposlFJK1UuThVJKqXppslBKKVWvFtMbSimlQuVwcQXLt+eRV1xBx+Q4uraNJ6NNPMlx0UFfd0FJJTvzi9mZV8LOPPs5v4RdeSX884rBDM1sG5T1arJQSql6VFa7+G7XET7fnMsX2bn8sKcAp5GSUuOj6ZoWbz8SPJ79SybGGA4WlrMzr4QdecXsOpYMitmRV0JBaaXPZXfklWiyUEqpUNpxqJgvsnP5bPMhlm3Lo6i8/htLFpRWUlBaybq9Rx2n1ySTjDbHk0h0pBw7O9iZV8yu/BLKKl2Oy9dnV15x/TM1kiYLpZQCjpZV8s3WPPvs4RC78kvqX6iB6ksmJ2pnEGKuoclCKdUqVbsMP+4pOFa19O2uI1S7mvco3DvzNFkopVSDFZdXsa+glH0FZew7UmY9F5Syt6CMH3KOcKTEd/1/fXqlJzIgI5VDReXkHC5l75FSqkKQbKIjhW5pCfRol0CPdol0b3v8dde0+KCtV5OFUqpZKqmoYu+RMvYXlLG3oJT9NYnAraywrP52Bn8lx0VxZp90zspqz1lZ6XRrm1BrerXLcLCwjJzDpeQcLiEnv9R6faSEPYdL2XOklMpq/5JJUmzUsSTQvV0Cme0S6dHWet05NZ7ICAnY9/KXJgulVEBUuwylldWUlFdRXFFNcXkVpZXWc0lFtf2wXldUuaxHtcdzlYtKzzKHaUXlVRwNYCJwEiEwqFsbzspqz9l923Na11SiIn1fmhYZIXROjadzajzDHHokVbsMuYXlViKpSSiHrbORrmnxVmJom0iPdgm0S4xBJPQJoS6aLJQKkkNF5WzcV8i2Q0UYA20SommTEEOb+GjSEmJITYgmJS4qoAeFiioXR0oqyC+pIL+4gsPFleSXVHC4uILDJRVUuwwuY3AZMMbqpun03gAuAy5jwH52GUO1C8oqqymuqKKkvJqSSuu5uKKq0T14mpKMNvGc3Teds7PaM6p3OqkJgbtuIjJC6JQaR6fUOIZmBuxjQ0aThVInqLSimuyDhWzcX8im/YVs3H+UTfsLOVRUUe+ykRFCanw0beKjjyeThGjaxFvPaQnRpNoJprSymsPFFccO/vnFlRyuSQolFeQXVVDoR/dOdVx8dCRn9G7H2VnpnNW3Pb3SE5vcL/qmQpOFUn5yuQy78kvY6JYQNu0vZEdeMY1t16x2GfKLrQO+CryoCKFjShydU+Po3CbeerYfXdrEc1KnZGKjIsMdZrMQ1GQhIhOBJ4FI4HljzCMO81wG3A8Y4HtjzBVu01KADcBcY8zNwYxVqRrGGA4cLSf7YCHZB4qOnS1sPlBEaWV1uMNTNvdE0Mk++HdKiaNLmzg6pcbTJTWOdkmxYWkMbomClixEJBJ4GhgP5AArReQDY8x6t3mygHuA0caYwyLSweNj/gR8FqwYVevmchn2FpSSfbCILQeKrORgv9bqnMZJiIm0H1EkxESSGBt1rCwxJoqE2EjioyOJjYokJiqCmKgIoiOt59jI2u+t10JsVAQxkZHH3sdERRAbFUnbxBhNBCEUzDOL4cAWY8w2ABF5E7gQWO82z/XA08aYwwDGmIM1E0RkCNAR+BgYGsQ4VQtX7TLszi8h+6CVELYcLDr2KKkI3plCdKTQu30S/TolkxgbxZGSSo6UWo3OBaVWe0Og1y8CaQkxpCVE0zYxhrSEGNolxdhlMcRGRyCAiBAhgojV60eoeV372ZrPfm8vFx8TSaKdEBJjI+33UcRHRxKhB+8WK5jJIgPY7fY+BxjhMU9fABH5Cquq6n5jzMciEgE8BlwFjPO1AhGZDkwH6N69e+AiV2FVUeVi1Y58co6UUlVtqHK5qKw2VFW7qHIZKqtdVFUbKl3Wc1W1i0qXPb3aHHtdWe1iz5EytuYWUVEV3J46GW3i6dcpmZPsR//OKfRMTyS6jq6WAOVV1RSUVlqJpMRKIAX285Fj5RUUlFYSGxVB28RY2iZGk5YYQ9uEGOu5JikkxpASH62/tlVQBDNZOO2xns2AUUAWcA7QFfhCRAYAVwLzjTG76+qZYIx5DngOYOjQoc37Ov1WzhjDt7uOMO+7Pfz3h70cPoEra4MpOS7KLSmk0L9TMn07JZPSyKGpY6Mi6ZAcSYfkuABHqlRgBTNZ5ADd3N53BfY6zLPMGFMJbBeRTVjJ4wzgLBH5DZAExIhIkTHm7iDGq8Jg+6Fi5n23h3lr9gR1XJuGio+OpE+HpGOP/p2t5NAlNU67VqpWKZjJYiWQJSI9gT3AVOAKj3nmAdOAl0QkHataapsx5hc1M4jItcBQTRQtR35xBf/9YS9zv9vDd7uOhDWWpNgo+nRIIqtDElkdk8jqkEyfDklktInX+nel3AQtWRhjqkTkZmABVnvEbGPMOhF5EFhljPnAnjZBRNYD1cCdxpi8YMWkwqessprFGw4w77s9fLopNyQDrrlLiYuib8dksjom0adD8rHk0ClFzxSU8ocYp9s9NUNDhw41q1atCncYyo3LZVi2PY953+3hox/3N6g7amxUBGP6tqdtYgxRkUJUhNVtMioygugI6zkqUoiOsJ7dy6Pt+aMiheQ468yhfVKsJgWlHIjIamNMvT1O9QpuFXDZBwp599s9vL9mD/sKyvxeTgRG9W7HRYMymDigU0juZ6yU8o8mCxUwFVUu/vTf9by6bGeDluvXKZkpgzO4cFAGnVK1V5BSTZEmCxUQh4rKufG11azccdiv+TumxHLRoAwuGpxB/84pQY5OKXWiNFmoE7ZubwHTX1nNniOldc6XFBvFxAGduHhwBiN6tdOLx5RqRjRZqBPy4Q/7uOOd730OsBcZIYzp254pgzP4Sf+OxMfoCJ9KNUeaLFSjuFyGJ5Zk89SSbMfpURHCb8f3ZeqwbrRLig1xdEqpQNNkoRqsuLyK3729hgXrDjhOT0uIZuaVQxjZq12II1NKBYsmC9Ugu/NLuP6VVWzcX+g4vV+nZGZdPdTrZvZKqeZNk4Xy2zdb8/jN66t9DvJ33ikdefyyQSTG6m6lVEuj/9XKL68u28kDH6zzOUzHbeOyuG1clo6npFQLpclC1amiysUD/1nH68t3OU6Pj47ksctOY/LAziGOTCkVSposlE95ReXc+Pq3rNie7zg9o008z109hFO6pIY4MqVUqGmyUI427DvKdS+v8nmh3fDMtjxz5emka7dYpVoFTRbKy8dr9/G7t7/3eX/oacO78cAFA4iJqvuWoUqplkOThTrG5TI8tTSbJxY7X2gXGSHc97OTuWpkDx3uW6lWRpOFOuaxRZt4+pOtjtPaJETzzBWnM6pPeoijUko1BZosFAA784qZ+alzoujbMYnnrx5G93Z6oZ1SrZUmCwXAhz/uw+kSip/078gTUweRpBfaKdWq6RFAAfDRj/u9yqYN78bDFw3UC+2UUmh3FsXu/BJ+3FPgVT797N6aKJRSgCYLBXy0dp9XWf/OKfRMTwxDNEqppkiThWK+QxXU5AGdwhCJUqpRDu+A3M1BXYUmi1Zuz5FS1uw+4lU++VQd60mpJm/vd/DOL+GpwbDwD0FdlTZwt3If/ehdBXVSx2R6t08KQzRKqXoZA1uWwNdPwvbPj5dnL4CDG6BD/6CsVpNFK/fRWu8qqEkDtQpKqSanuhLWvgtfPQUH1znP8/U/4KJngrJ6TRat2P6CMlbvPOxVrsONqxYtfxssexbyt0KP0TDsOohLCXdUvpUXwuqXYdkzcHRP3fP+8Dac+wdI6RLwMDRZtGIfO/SC6t0+kawOWgWlWiBj4Ps5MP9OqCiyyrYshq+fgtG3w/DpENOERiko3A/Ln4WVs6Hcu2u7l8QOMHIGxASnF6Mmi1ZsvkMV1OSBnXWQQNXylB6BD39nVeN4TTsMi++zfrmfdQcMuQaiwjj0fu4mK4H98DZUV9Q/f7ssGHULnHo5RMcFLSxNFq3UwaNlrNzhfVMjrYJSLc6u5fDudVDgfLfHY4oOwEd3WvX+Y/4HTpsGkSE6RBoDu5bBV0/C5o/8W6bbSBh9K/SdBBHB79iqyaKVWrBuP8ZjLKie6Yn065QcnoA8VRRbz0E6pVatQHUVfPEYfPYIGJf/yxXsgg9uhq+egLH/CydPCd7B2FUNGz+0ziRyVvqxgEC/82HUrdB9RHBi8kGTRSvldCHepAGdwl8F5XLB53+1fmFVlkCHU+CUKdYjvU94Y1PNx5Fd8N502PWN73kyhsLeb30nkrwt8O9fQce/w7m/h74T4UT/P4yBIzth9wrrTGLrUji8vf7lImNh0DQ442ZIzzqxGBpJk0UrdKionOXb87zKw14FVVUOc2fAuveOlx1cZz0+eQg6DoRTLrISR7ve4YuztXK5rIbhmKSQVHs02tr34D+3+24UjkmGnz4Op14Gh7Lhk/+rvc95OvAjzJkKXYdZPY16neN/LNWVsO8H2L0cdi+zqsSKvH+o+RSXCsOuhxE3QFIH/5cLAjGedRGB/HCRicCTQCTwvDHmEYd5LgPuBwzwvTHmChEZBMwEUoBq4GFjzFt1rWvo0KFm1apVAf4GLdMby3fxv3N/rFXWrW08n985NnxnFqWH4c0rYeeX/s3f6dTjZxxtewY3tpbMGGvbFx2AooPWo/ig/T7X43UumGqIToT2faF9f+jQ7/hzarcT/+V9IsqL4KO7YM1rvufpOgwunuW9z+z7AT55GDZ/XP96Ms+CcfdCt+He00ryreqk3cutxLBnNVQ538e+Tqnd4IybYPBVEBvc3okistoYM7Te+YKVLEQkEtgMjAdygJXANGPMerd5soC3gXONMYdFpIMx5qCI9AWMMSZbRLoAq4H+xhjvcSlsmiz8d9ULy/ki+1CtshvG9OKeScG58rNeBTnw2s8hd0Pjlu8y2EoaJ18EaT0CG1tLUNN4unUpHN3rnQxcVYFZT0wStD/JO4mkZAQ/iez51mrEzne+gRcSYfV0GvM/EBnt+3N2r4SlD9a+MtqXrPOsA3pBjn3msBxyNzYu/hqdBsKo26wz6LriDKCmkCzOAO43xpxnv78HwBjzZ7d5/gpsNsY8X89nfQ/83BjjfHNoNFn4K7+4gmEPL6ba405H7980mtO6tQl9QAfWWYmicG9gPi9jyPHE0aZbYD6zuaqqgHVz4Zt/wv4fwhdHbIqVRDr0P55A0vsGJom4XFbj8NI/+U56KRnW2UTmaP8/d9tn1mf61egcAL3GWj2beo0N+dmZv8kimG0WGcBut/c5gGfzfV8AEfkKq6rqfmNMrfNAERkOxAA+fjKohli0fr9XoshoE8+pXVNDH8y2z+CtK6H8qPe0iCiY+AhUlloHvL3f+veZe1Zbj4V/sKocsibYdexRVj17RJTHI7L+9y4XVJVZbSpVZVBdfvx1rWen8jKr3jqls9VA2vvc4PfhL8mH1S/CillQ6H3hZciVH7UOup4H3uhEq+0pPcu6ViA9C9r1sR7+VL0c3Qtzb6j7LODkC+FnT0J8WsNi7jUGep4NmxfA0oesdotAiYiGLoOg2wjoPtJ6DnN7hD+CmSyc0qPnaUwUkAWcA3QFvhCRATXVTSLSGXgVuMYY7y4LIjIdmA7QvXv3wEXegjWZXlA/vAPzbgRXpfe0mGS4/BXrwArWL67DO2DdPCtx7Fvj3zqcDlDhtPol61d2v/Ots59eYyEqJnCff2iLdWHZmjcaV0/uj8gY/y4U80dlsXXG43TWk5JhJY1jiaSP9ZzazUr6Gz+E92+GUu9rhQCIToBJf4XBVzb+l7oInDTR+sGxfp7VEJ7ns3LDt/i2dmIYYV0b0WVwUC+eC5ZwV0M9Cywzxrxkv18C3G2MWSkiKcCnwJ+NMe/Utz6thqpfQUklQx5aRJXHmcW7N45iSI8G/vJqLGOsaoNF9zpPT+oEv3gHOp/q+zPytx1PHOGsXjlRcanQ72d24hjTuDpqY2DHF/DN0/41zjqJSbJ+2SZ2sJ6TOkBSR0hsbz3XlCW2h6g4a3yigxutNqaa59xNx4fQCKaoeKt68VAd927ofBpc8kLgu5hWV8EPb8Knf6n7Ar/0vlbjd7eR1plDuz7hbfivR1Nos4jCauAeB+zBauC+whizzm2eiViN3teISDrwHTAIKAQ+Av5jjHnCn/Vpsqjfv1fncMc739cq65QSx9d3nxua26e6quHje2DFv5ynp/eFK9+FNg04Szy0BdbPtZLHgbWBiTMc4tOg30+txNHz7PoTR1WFNXTFN0/7X0WSMdTqLprSxS0ZdAjMhY8uFxzNcU4ilSUn/vn+GnUrnPvHwJ6xeaoqh29fgeX/sjoIdDjFPmsYAV2HQ2K74K07CMKeLOwgJgNPYLVHzDbGPCwiDwKrjDEfiFX38RgwkeNdZN8UkSuBFwH3cXivNcb4rH/QZFG/X7+0kiUbD9Yq++XoTO772SnBX3llKbx3PWz4j/P07mfA1DcgoW3j15G72aouWDcXDq6vf/6mKr4t9P8ZDLgYepxZe8iJ4jxYPdtqjyg6UP9nSYT1WWfc7NzVM9hcLutX+MGN1t8kd6OVQPK2BPZMJKkjTHn2eNWl8luTSBahpMmibkfLKhn6p8VUVNdu+nlnxhkMyzyBA7Q/SvJhzjTroiQn/S+weqsEsh734EbIXmgdUF3VVk+ZYw+H96baeXp1pdXQHRVrP+KOP0fG1H5f69ntdXWl1VC6aX7DD5AJ6XDyBdDnJ5C9yBo1taqs/uVikuH0q2HEdEjLbNQmDCpjrFFVD2222gEObbGfs62rr72aN+vQdyJc+DQkpgct3JYsoL2hRCQN6AKUAjucGptV07ZkwwGvRNEhOZYh3YPcVnF4J7z+c991zCNmwHn/Z/U6CqQO/axHUzHw59bZ1ZYl1pnPpo+sBt76lByCVbOthz9Su1vDVA++qmnfo0HE6iGW0tlqr3FXWWq1Sx3K9kgkW2pflR0VDxP+ZN2Pogm3CbQUPpOFiKQCNwHTsLqu5gJxQEcRWQY8Y4z5JCRRqhPm1Atq4oBOwW2r2Pc9vH6p7+qS8X+yhlZuLf/o0fHQ/6fWo7LUOlNYN9dqmD7Rev2uw60LxPr9NHQjpQZLdDx0PMV6uDPGusI8LxvKjlrVano2ETJ17VX/Bl4BzvK8clpEhgBXiUgvY8wLwQxQnbii8io+25zrVT5pQBDHgtq6FN66yrnaJSLaql8e+PPgrb+pi463qpdOvsAaYTd7oZ04Fvrf7VUireVH3gTdhgU33qZABJI7Wg8Vcj6ThTFmfB3TVmMNwaGagaUbD1JRVbsKKj0phuE9g9RWsWaONcSz0xW1sakw9TWrx4+yxCQeH+eqvAiyF1iJI3uRc/tEbIrdHnFDw3qOKXUC6j1fFZF3gdnAR9pW0TzN/8H7Kt7zTulEZKCqoIyxerpsXmAd4HZ97Txfche48t/e1QvquNgkGHCJ9SgvtLbp2vescYeSO1kXmQ2+EmKbyH1HVKvhT+XmTOCXwFMi8g7wkjHmBEfLUqFSXF7FJ5sOepWf8HDk5UXWMAvZdoKo70by7ftbiSK164mttzWJTbaq6lpzdZ1qMupNFsaYxcBiu8F7GrBIRHYDs4DXjDEO4zWopuLTTbmUe1RBpSVEM6IxVVB5W6269c0LYOdX/g/7kHkWXP4axIdhoEKlVED423W2HXAlcBXWVdavA2cC12CN66SaqPlrnaugoiL9uHlNVTns+NI6c8he6Hv457qccrHVmB3swfOUUkHlT5vFe0A/rAH9fmaMqTn6vCUiehVcE1ZaUc0nG72roCbVVQVVeMC6eCx7oTUqrD/XAjhp0x3OuMXqA9+U76qmlPKLP2cW/zTGLHWa4M9Vfyp8PtucS0lFda2y1PhoRvX2MXbNd6/Dh/+vcSOWRkRBj1HWCJ1Z51mDuLWW6yeUagX8SRb9ReRbt2HD07AG/3smuKGpEzX/R+8qqAkndyTaqQqq+BDMv6NhiSKpI2SNt5JDr3Oa9hXDSqkT4k+yuN4Y83QJUQ+pAAAgAElEQVTNG/v2p9cDmiyasLLKapZs8L5y2mcvqOyFflxFLMdvKNR3AnQcqFVMSrUS/iSLCBERY484aN9bO4jj/6pA+CL7EMUeVVDJcVGM6uOjCmrzAufy+DRrELusCdB7XLMbflkpFRj+JIsFwNv2jYoMMANo5F1WVKh85FAFNb5/R2KjHAbsq66ErQ7DfF00E069PPCD/Cmlmh1/ksVdwA3AjVi3Sl0IPB/MoNSJKa+qZlFDqqB2r6g9midYQ1wP+LkmCqUU4N9FeS6sq7hnBj8cFQhfbTlEYVntcZmSYqM4M8vHCJ3ZDlVQvc8J7t3GlFLNij/XWWQBfwZOxhqiHABjTK8gxqVOgNNw5OP6dyAu2sdZQvYi77Ks8wIclVKqOfOnK8uLWGcVVcBYrGHLXw1mUKrxKqpcLFznnSx8Dkd+ZLfzLUizfA46rJRqhfxJFvHGmCVYt2DdaYy5H9Ab3TZR32zL46hHFVRCTCTnnNTeeYHshd5lnU+zRjhVSimbPw3cZSISAWSLyM3AHqBDcMNSjeXUC+rcfg2tgpoQ4KiUUs2dP2cWtwMJwK3AEKwBBa8JZlCqcaqqXSxwqILy2Quqsgy2f+ZdrslCKeWhzjML+wK8y4wxdwJFWPe1UE3Usm35HC6pPWJ8XHSE7yqonV96X7Ud3xYyhgQpQqVUc1XnmYUxphoYIqIjwjUHTsORjz2pAwkxPn4TOFVB9fmJXluhlPLiT5vFd8D79l3yjo1XbYx5L2hRqQardhkWrHXoBeWrCsoY5yE++mqXWaWUN3+SRVsgj9o9oAygyaIJWbE9n7zi2neui4mK4Nx+Pvoi5G2Fw9trl0kE9NaObkopb/5cwa3tFE1MWWU1O/NK2H6omB15xew4VMyK7fle853Ttz1Jsb6qoBzOKroOg4RG3G5VKdXi+XMF94tYZxK1GGN+FZSIFGAlhN35xxPC9kMl7LBf7yso8+szfPaCAufrK7QXlFLKB3+qof7r9joOmALsDU44rZMxhk82HWTpxoNWcjhUwt6CUoxXivZfTGQE5/b3UQVVXgQ7vvIu12ShlPLBn2qod93fi8gcYHHQImqFnlySzROLswP6mRcN7kJKXLTzxG2fgqt2F1uSO0OngQGNQSnVcvhzZuEpC+ge6EBaq+92HebJJYFLFFERwvmndub355/seybHKqjxes9spZRP/rRZFFK7zWI/1j0u1AmqrHZxz3s/Nqq6qUtqHJnpiWSmJ9Kznf2cnkC3tgnONziqYYwO8aGUajB/qqGSQxFIa/Tc59vYuL/Q5/ROKXFkpifQMz2RzGMJIZHubRN8j/VUnwNrodCjySkiGnqd07jPU0q1Cv6cWUwBlhpjCuz3bYBzjDHz/Fh2IvAkEAk8b4x5xGGey4D7sc5evjfGXGGXXwP8wZ7tIWPMy359o2ZiW26RY/XTKV1S+Nulp5HZLpH4mCBcSe1UBdVjFMTqbwKllG/+tFncZ4yZW/PGGHNERO4D6kwW9rhSTwPjgRxgpYh8YIxZ7zZPFnAPMNoYc1hEOtjlbYH7gKFYSWS1vezhhn29psnlMtzz3o9UVLlqlUdGCH+55FT6d04J3so3OyQLvWpbKVUPf0addZrHnyQzHNhijNlmjKkA3gQu9JjneuDpmiRgjDlol58HLDLG5NvTFgET/Vhns/D2qt0sd7iI7tdn9mRARmrwVlySDzkrvMu1vUIpVQ9/ksUqEXlcRHqLSC8R+Tuw2o/lMoDdbu9z7DJ3fYG+IvKViCyzq638XbZZOni0jIfnb/Aq7942gd/+pG9wV751KZjaZzOkZUK7PsFdr1Kq2fMnWdwCVABvAW8DpcBNfizn1A/Ts99PFFZX3HOAacDzdpuIP8siItNFZJWIrMrNzfUjpPC7/z/rKPS4kx3Aw1MGBKeNwp1jl9nztMusUqpe/vSGKgbubsRn5wDd3N53xfvK7xxgmTGmEtguIpuwkkcOVgJxX/ZTh9ieA54DGDp06Alc7xwaC9ftZ/6P3iPDXnx6Bmdl+bjnRKC4qmGLw7WUWgWllPJDvWcWIrLI/rVf8z5NRBxGofOyEsgSkZ4iEgNMBT7wmGceMNb+3HSsaqltwAJggr2uNGCCXdZsFZZVcu/767zK2yXG8Me6LqALlD3fQkle7bKoeMg8M/jrVko1e/40VKcbY47UvHHvtVQXY0yVfc/uBVhdZ2cbY9aJyIPAKmPMBxxPCuuBauBOY0wegIj8CSvhADxojPFuEW5G/vrxJvYf9R4A8N6fnUxaYkzwA3Cqguo1BqLjgr9upVSz50+ycIlId2PMLgAR6YFD+4ETY8x8YL5H2b1urw3wO/vhuexsYLY/62nqVu3I59VlO73KzzmpPRec1iU0QTgNSZ41PjTrVko1e/4ki98DX4rIZ/b7s4HpwQupZSmvqubu9370Kk+IieShiwYQkjvWFu6Hfd97l2t7hVLKT/40cH8sIqcDI7F6Kf3WGHMo6JG1EDM/3cqWg0Ve5XdMOImuaQmhCcKpYbt9f2ij40Eqpfzj76iz1cBBrPtZnCwiGGM+D15YLUP2gUKe/mSLV/lp3dpwzajM0AXieK9tPatQSvnPn7GhrgNuw+q+ugbrDOMbat+TW3lwuQx3v/cjldW1m3eiIoRHLh5IZESIrm2oroStn3iXaxWUUqoB/Lko7zZgGLDTGDMWGAw0jyvgwuj15TtZvdN7KKsbxvQK7thPnnZ9AxUeI9vGpkK3EaGLQSnV7PmTLMqMMWUAIhJrjNkInBTcsJq3fQWl/OXjTV7lPdMTueXcrNAG49RltvdYiPRxFz2llHLgT5tFjn1R3jxgkYgcRu/B7ZMxhj/OW0dRufeQHn++eGDj70PRWHqjI6VUAPjTG2qK/fJ+EfkESAU+DmpUzdhHa/ezeMMBr/Kpw7oxsle70AZzeCfkbvQu1+srlFIN5DNZiEiSMaZWn09jzGf1zdOaFZRUct8H3kN6pCfFcs+k/qEPyKkKqstgSKr3AnyllKqlrjaL90XkMRE5W0QSawrtYcp/bY8P1WLuMREIf/5oA7mF5V7lD154CqkJYWgjcBxlVquglFIN5/PMwhgzTkQmAzcAo+0B/aqATcCHwDXGGO8hVFupZdvyeHPlbq/yn/TvyKQBnUIfUGUpbHe4FCZL74qnlGq4OtssnMZ2Ut7KKqu5x2FIj6TYKP500SmhGdLD044vocpj4MKEdKsaSimlGsifrrOqHv9Yms32Q8Ve5XdNPInOqfFhiAjnq7azxkOE/smVUg2nR44TtGHfUf712Tav8iE90vjFiB5hiAgwRkeZVUoFlCaLE1BWWc1v31pDlav2kB7RkdaQHhGhGtLD06FsOLKrdplEQm8doUUp1Th+JQsROVNEfmm/bi8iPYMbVvPwp/+uZ+P+Qq/y35zTh6yOyWGIyOZ0VtFtBMSnhT4WpVSL4M9tVe8D7gLusYuigdeCGVRz8N8f9vL68l1e5X06JPGbsb3DEJEbxy6zWgWllGo8f84spgAXAMUAxpi9QBh/Noffzrxi7n7Xu/dTTFQET04dRGxUiIf0cFd2FHZ+7V3eV7vMKqUaz59kUWHf/tQAuF+g1xqVV1Vz0xvfOo799MefnswpXVLDEJWbbZ+CyyO2lAzocHJYwlFKtQz+JIu3ReRfQBsRuR5YDMwKblhN15/nb2TtnqNe5ecP7MyVI5rAned8VUGF41oPpVSL4c9Agn8TkfHAUayhye81xjgMZdryfbx2Py99vcOrvHvbBP58ycDwXHznzhgfo8xqFZRS6sTUmSxEJBJYYIz5CdAqE0SN3fkl/M+/v/cqj44U/nnFYFLimsD9Ifb/AEUeI7BExkDPs8MTj1KqxaizGsoYUw2UiEiYK+LDq6LKxc1zvuNomXc7xf9O7s+pXduEISoHmx2qoDLPhNik0MeilGpR/Ln5URnwo4gswu4RBWCMuTVoUTUxjy7YyPe7j3iVTzi5I9eOygx9QL7oKLNKqSDxJ1l8aD9apSUbDjDri+1e5Rlt4nn056eFv52iRnEe5Kz0LtdkoZQKAH8auF8WkRigr120yRhTGdywmoa9R0r5f+94t1NERQj/uGJweO5R4cvWJdi9m49r2xvahfkCQaVUi1BvshCRc4CXgR2AAN1E5BpjjMPNElqOqmoXt875jiMl3nnxfyaexOndm9jQGVoFpZQKIn+qoR4DJhhjNgGISF9gDjAkmIGF2+OLNrNq52Gv8rEntee6M3uFIaI6rJgFa9/zLu+ryUIpFRj+JIvomkQBYIzZLCJNqP4l8D7bnMszn271Ku+UEsdjlw0K32iynqor4aP/gVWzvadFJ0CP0aGPSSnVIvmTLFaJyAvAq/b7XwCrgxdSeB04Wsbv3lrjVR4ZITw1bTBtE2PCEJWDknx4+2rY8YXz9CG/hKjY0MaklGqx/EkWNwI3AbditVl8DjwTzKDCpdpluO3N78grrvCa9rvxfRnes20YonJwcAPMmQqHdzhP73EmnPv7kIaklGrZ/EkWUcCTxpjH4dhV3S3yJ+tTS7JZti3fq/ysrHRuHNNEehVt+hjevQ4qvO+jAVhnFJP+ClFN5AxIKdUi+DOQ4BLA/UbS8ViDCbYoX285xFNLs73K2yfH8nhTaKcwBr58wjqjcEoUEgmT/wY//bsmCqVUwPmTLOKMMUU1b+zXCf58uIhMFJFNIrJFRO52mH6tiOSKyBr7cZ3btL+KyDoR2SAiT0kQr37LLSzntrfWYDwuUxCBJ6cOon1ymE+kKstg7gxYfB9e11IAxLWBq96D4dfr6LJKqaDwpxqqWERON8Z8CyAiQ4DS+hayq6ueBsYDOcBKEfnAGLPeY9a3jDE3eyw7ChgNnGoXfQmMAT71I94GcbkMv3t7DbmF5V7Tbj03i1G90wO9yoYp3A9v/gL2rHKent4Xpr2pF98ppYLKn2RxO/COiOy133cGLvdjueHAFmPMNgAReRO4EPBMFk4MEAfEYDWqRwMH/FiuwWZ+tpUvsg95lY/s1ZZbx2UFY5X+2/sdzLkCCvc6T+/zE/j5bIhr1eM8KqVCwJ/hPlaKSD+se1kIsNHP4T4ygN1u73OAEQ7zXSIiZwObgd8aY3YbY74RkU+AffY6/2mM2eDHOhtkxfZ8Hlu4yau8XWIMT04dTGQ42ynWvgvzboIqHydxZ9wM4x+EiDDewlUp1Wr4bLMQkWEi0gnATg6nAw8Bj4mIP31InY60nhXu/wEyjTGnYjWav2yvuw/QH+iKlXTOtROKZ4zTRWSViKzKzc31I6Tj8osruHXOd7gcmgD+fvkgOqbENejzAsblgqUPw79/5ZwoImPgwmfgvIc1USilQqauBu5/ARUA9oH6EeAVoAB4zo/PzgG6ub3vCtSqTzHG5BljahoLZnF8CJEpwDJjTJHdoP4RMNJzBcaY54wxQ40xQ9u3b+9HSMc9tnAT+4+WeZX/5pzenN23YZ8VMOVF8PZV8Plfnacntodr/guDfxHauJRSrV5dySLSGFNz0cHlwHPGmHeNMX8E+vjx2SuBLBHpaY9aOxX4wH0GEens9vYCoKaqaRcwRkSi7KFFxrhNC4i7J/Xjp6d2rlU2tEcavxvf18cSQXZkF8w+Dzb+13l6x4Fw/SfQ3akmTymlgqvOZCEiNW0a44ClbtP8aeuoAm4GFmAd6N82xqwTkQdF5AJ7tlvt7rHfY10hfq1d/m9gK/Aj8D3wvTHmP35+J78kx0Xzj2mD+b8pA4mNiqBNQjRPTRtMVKQ/vYkD7OAGeG4sHFjrPL3/BfDrBdCmm/N0pZQKMjGeFxfUTBD5PTAZOAR0B043xhi7PeFlY0yTGqVu6NChZtUqH91L67Fh31HyiysY3ScM3WRdLnh2NBz00UlszF0w5m6ICEMSU0q1eCKy2hgztL75fJ4hGGMeFpElWF1lF5rjWSUCuCUwYTYN/TunhG/lmz92ThRR8TBlJpwyJfQxKaWUhzqrk4wxyxzKNgcvnFbo66e8y1IyYOob0GVQ6ONRSikHWrcRTrtXwq5vvMsveEoThVKqSdFkEU7f/MO7rMMp0Htc6GNRSqk6aLIIl/xtsMGhg9eoW3QwQKVUk6PJIly+eQaMq3ZZcmcYcEl44lFKqTposgiHknz47jXv8hEz9F4USqkmSZNFOKx8wXvcp5gkGHJtWMJRSqn6aLIItcoyWPEv7/Ih10J8m5CHo5RS/tBkEWo/vAnFHiPkSqRVBaWUUk2UJotQcrng6396lw+4WMd9Uko1aZosQil7AeRle5efcbN3mVJKNSGaLELpa4eL8HqerVdrK6WaPE0WoZKzGnZ+5V0+6rbQx6KUUg2kySJUnAYM7HAy9NGhPZRSTZ8mi1DI3w4bPvAuP+NmHdpDKdUsaLIIhWUzvYf2SOoEA38enniUUqqBNFkEW0k+fPeqd/nIGRAVG/p4lFKqETRZBNuqF6CypHZZTBIM+WV44lFKqUbQZBFMlWWw/Dnv8tOv1qE9lFLNiiaLYPrxbSg+WLtMImHkjeGJRymlGkmTRbC4XM4X4Z0yBdp0D308Sil1AjRZBEv2Qji02bt8lA7toZRqfjRZBIvTWUXmWdBlcOhjUUqpE6TJIhj2rIadX3qXj7o19LEopVQAaLIIBqezivb9IGt86GNRSqkA0GQRaId3wPr3vctH3aJDeyilmi1NFoHmOLRHRxh4aXjiUUqpAIgKdwAtSkk+fOswtMeIG3RoD6XqUVlZSU5ODmVlZeEOpUWKi4uja9euREdHN2p5TRaBtPpFqCyuXRadCEN/FZ54lGpGcnJySE5OJjMzE9Eq24AyxpCXl0dOTg49e/Zs1GdoNVSgVJXD8n95l59+NcSnhT4epZqZsrIy2rVrp4kiCESEdu3andBZmyaLQPnhbSg6ULtMInRoD6UaQBNF8Jzotg1qshCRiSKySUS2iMjdDtOvFZFcEVljP65zm9ZdRBaKyAYRWS8imcGM9YT4Gtrj5IsgrUfo41FKqQALWrIQkUjgaWAScDIwTUROdpj1LWPMIPvxvFv5K8Cjxpj+wHDgoMOyTcOWxXBok3f5qFtCH4tSqlGOHDnCM88806hlJ0+ezJEjRwIcUdMSzDOL4cAWY8w2Y0wF8CZwoT8L2kklyhizCMAYU2SMKalnsfBxur925lmQcXroY1FKNUpdyaK6urrOZefPn0+bNuG77UB98QVCMHtDZQC73d7nACMc5rtERM4GNgO/NcbsBvoCR0TkPaAnsBi42xgT/C3SUHu+hR1feJfrWYVSjZJ594dBX8eOR873Krv77rvZunUrgwYNYvz48Zx//vk88MADdO7cmTVr1rB+/Xouuugidu/eTVlZGbfddhvTp0+3Ys7MZNWqVRQVFTFp0iTOPPNMvv76azIyMnj//feJj4+vta533nmHBx54gMjISFJTU/n888+prq7mrrvuYsGCBYgI119/PbfccgtLlizhjjvuoKqqimHDhjFz5kxiY2PJzMzkV7/6FQsXLuTmm29m2LBh3HTTTeTm5pKQkMCsWbPo169fwLZZMJOFU2uK8Xj/H2COMaZcRGYALwPn2nGdBQwGdgFvAdcCL9Ragch0YDpA9+5hGPa7qhwW/sG7PP0k6KNDeyjVnDzyyCOsXbuWNWvWAPDpp5+yYsUK1q5de6y76ezZs2nbti2lpaUMGzaMSy65hHbt2tX6nOzsbObMmcOsWbO47LLLePfdd7nyyitrzfPggw+yYMECMjIyjlVfPffcc2zfvp3vvvuOqKgo8vPzKSsr49prr2XJkiX07duXq6++mpkzZ3L77bcD1rUTX35pjUM3btw4nn32WbKysli+fDm/+c1vWLp0acC2TzCroXKAbm7vuwJ73WcwxuQZY8rtt7OAIW7LfmdXYVUB8wCvOh1jzHPGmKHGmKHt27cP+Beok8sFc2fAzq+8p426BSK0o5lSzd3w4cNrXZfw1FNPcdpppzFy5Eh2795Ndna21zI9e/Zk0KBBAAwZMoQdO3Z4zTN69GiuvfZaZs2adawKafHixcyYMYOoKOs3fNu2bdm0aRM9e/akb9++AFxzzTV8/vnnxz7n8ssvB6CoqIivv/6aSy+9lEGDBnHDDTewb9++wGwEWzDPLFYCWSLSE9gDTAWucJ9BRDobY2q+0QXABrdl00SkvTEmF+tsY1UQY224hX+Ade95lyd3hlMvC308SqmAS0xMPPb6008/ZfHixXzzzTckJCRwzjnnOF63EBt7fLSGyMhISktLveZ59tlnWb58OR9++CGDBg1izZo1GGO8urca41kZ4xyfy+WiTZs2x86KgiFoP3/tM4KbgQVYSeBtY8w6EXlQRC6wZ7tVRNaJyPfArVhVTdhtE3cAS0TkR6wqrVnBirXBvv4HLHvauzwiCi58Wof2UKoZSk5OprCw0Of0goIC0tLSSEhIYOPGjSxbtqzR69q6dSsjRozgwQcfJD09nd27dzNhwgSeffZZqqqqAMjPz6dfv37s2LGDLVu2APDqq68yZswYr89LSUmhZ8+evPPOO4CVZL7//vtGx+ckqMN9GGPmA/M9yu51e30PcI+PZRcBpwYzvkb54W3ndgqAC/4BfcaFNh6lWhinxudQaNeuHaNHj2bAgAFMmjSJ88+vHcfEiRN59tlnOfXUUznppJMYOXJko9d15513kp2djTGGcePGcdpppzFgwAA2b97MqaeeSnR0NNdffz0333wzL774IpdeeumxBu4ZM2Y4fubrr7/OjTfeyEMPPURlZSVTp07ltNNOa3SMnqS+05zmYujQoWbVqiDXVG1dCq9fBq5K72nj7oOzfhfc9SvVgm3YsIH+/fuHO4wWzWkbi8hqY8zQ+pbVVlh/7V0Db13lnCiGT4czfxv6mJRSKkQ0Wfgjfzu8filUFHlPO/lCmPiI3thIKdWiabKoT/EheO0SKHYYbaTHaJjyHEREhj4upZQKIU0Wdakots4o8rd6T+twMkx9A6LjQh+XUkqFmCYLX6or4Z1rYe+33tNSMuAX/4b48I0Fo5RSoaTJwokx8J/bIHuh97S4NnDle5CaEfq4lFIqTDRZOFn6EKx53bs8Kg6mvQkdAjc4l1KqaTiRIcoBnnjiCUpKmu7g2CdKk4WnFbPgi795l0sEXPI89Dgj9DEppYKuuSQLYwwulyvo6/EU1Cu4m531H8D8O52nTf4b9P9ZaONRqjW6PzUE6yjwKvIcovzRRx/l0Ucf5e2336a8vJwpU6bwwAMPUFxczGWXXUZOTg7V1dX88Y9/5MCBA+zdu5exY8eSnp7OJ5984vXZH3zwAVFRUUyYMIG//e1vHDhwgBkzZrBt2zYAZs6cyahRo3j88ceZPXs2ANdddx233347O3bsYNKkSYwdO5ZvvvmGefPmsWnTJu677z7Ky8vp3bs3L774IklJSUHbZJosauz8Gt69Du9R1IGz74Rhvw55SEqp0PEconzhwoVkZ2ezYsUKjDFccMEFfP755+Tm5tKlSxc+/NC670ZBQQGpqak8/vjjfPLJJ6Snp9f63Pz8fObOncvGjRsRkWNDkt96662MGTOGuXPnUl1dTVFREatXr+bFF19k+fLlGGMYMWIEY8aMIS0tjU2bNvHiiy/yzDPPcOjQIR566CEWL15MYmIif/nLX3j88ce59957CRathgI4uAHmTIXqcu9pg6+Esb8PfUxKqbBauHAhCxcuZPDgwZx++uls3LiR7OxsBg4cyOLFi7nrrrv44osvSE2t+0woJSWFuLg4rrvuOt577z0SEhIAWLp0KTfeeCPAsZsgffnll0yZMoXExESSkpK4+OKL+eIL6+ZqPXr0ODYe1bJly1i/fj2jR49m0KBBvPzyy+zcuTOIW0PPLKAgx7rorsz7tJSs8+CnT+rV2Uq1QsYY7rnnHm644QavaatXr2b+/Pncc889TJgwoc5f9FFRUaxYsYIlS5bw5ptv8s9//tPnTYnqGqvPfbh0Ywzjx49nzpw5DfhGJ6Z1n1mUHobXfg5H93hPyxgCl74IkZpPlWoNPIcoP++885g9ezZFRdYwP3v27OHgwYPs3buXhIQErrzySu644w6+/fZbx+VrFBUVUVBQwOTJk3niiSeOVXONGzeOmTNnAtY9tI8ePcrZZ5/NvHnzKCkpobi4mLlz53LWWWd5febIkSP56quvjg1dXlJSwubNmwO7QTy03iNhZRnMuQJyN3hPa9sbrngbYhK9pymlgsuh8TkUPIcof/TRR9mwYQNnnGH1gExKSuK1115jy5Yt3HnnnURERBAdHX3sgD99+nQmTZpE586dazVwFxYWcuGFF1JWVoYxhr///e8APPnkk0yfPp0XXniByMhIZs6cyRlnnMG1117L8OHDAauBe/DgwV5322vfvj0vvfQS06ZNo7zcqj5/6KGHjt1RLxha7xDl/7kdVr/oXZ7UEX69ENIyAxabUqp+OkR58OkQ5Y1x5u3WGYS7mGT4xTuaKJRSykPrTRZpmfDrRZBhJ9SIaLj8VegcuDtLKaVUS9F6kwVAYju45gPoOwkumgm9x4Y7IqVatZZSLd4Unei2bb0N3DViEmHaHO0eq1SYxcXFkZeXR7t27RD9fwwoYwx5eXnExTX+lgqaLEAThVJNQNeuXcnJySE3NzfcobRIcXFxdO3atdHLa7JQSjUJ0dHR9OzZM9xhKB9ad5uFUkopv2iyUEopVS9NFkopperVYq7gFpFc4ESGXUwHDgUonGDSOAOrucQJzSdWjTPwghlrD2NM+/pmajHJ4kSJyCp/LnkPN40zsJpLnNB8YtU4A68pxKrVUEoppeqlyUIppVS9NFkc91y4A/CTxhlYzSVOaD6xapyBF/ZYtc1CKaVUvfTMQimlVL00WSillKpXq0oWIjJRRDaJyBYRudtheqyIvGVPXy4imaGPEkSkm4h8IiIbRGSdiNzmMM85IlIgImvsh+87xgc31h0i8taeRrwAAAeRSURBVKMdg9etCsXylL1NfxCR08MQ40lu22mNiBwVkds95gnb9hSR2SJyUETWupW1FZFFIpJtP6f5WPYae55sEbkmDHE+KiIb7b/tXBFp42PZOveTEMR5v4jscfv7TvaxbJ3HiBDF+pZbnDtEZI2PZUO2TQFr6NrW8AAiga1ALyAG+B442WOe3wDP2q+nAm+FKdbOwOn262Rgs0Os5wD/bQLbdQeQXsf0ycBHgAAjgeVNYD/Yj3UhUpPYnsDZwOnAWreyvwJ326/vBv7isFxbYJv9nGa/TgtxnBOAKPv1X5zi9Gc/CUGc9wN3+LFv1HmMCEWsHtMfA+4N9zY1xrSqM4vhwBZjzDZjTAXwJnChxzwXAi/br/8NjJMwDKxvjNlnjPnWfl0IbAAyQh1HgFwIvGIsy4A2ItI5jPGMA7YaY07kav+AMsZ8DuR7FLvviy8DFzkseh6wyBiTb4w5DCwCJoYyTmPMQmNMlf12GdD4MbADxMf29Ic/x4iAqitW+9hzGTAnmDH4qzUliwxgt9v7HLwPwMfmsf8BCoB2IYnOB7sqbDCw3GHyGSLyvYh8JCKnhDSw4wywUERWi8h0h+n+bPdQmorvf76msD1rdDTG7APrxwPQwWGeprZtf4V1Fumkvv0kFG62q8tm+6jWa2rb8yzggDEm28f0kG7T1pQsnM4QPPsN+zNPyIhIEvAucLsx5qjH5G+xqlJOA/4BzAt1fLbRxpjTgUnATSJytsf0JrNNRSQGuAB4x2FyU9meDdGUtu3vgSrgdR+z1LefBNtMoDcwCNiHVb3jqclsT9s06j6rCOk2bU3JIgfo5va+K7DX1zwiEgWk0rjT2RMmItFYieJ1Y8x7ntONMUeNMUX26/lAtIikhzhMjDF77eeDwFysU3l3/mz3UJkEfGuMOeA5oalsTzcHaqrr7OeDDvM0iW1rN6z/FPiFsSvTPfmxnwSVMeaAMabaGOMCZvlYf5PYnnDs+HMx8JaveUK9TVtTslgJZIlIT/sX5lTgA495PgBqepT8HFjqa+cPJruu8gVggzHmcR/zdKppTxGR4Vh/y7zQRQkikigiyTWvsRo713rM9gFwtd0raiRQUFO9EgY+f6k1he3pwX1fvAZ432GeBcAEEUmzq1Um2GUhIyITgbuAC4wxJT7m8Wc/CSqPdrIpPtbvzzEiVH4CbDTG5DhNDMs2DVVLelN4YPXM2YzV4+H3dtmDWDs6QBxWFcUWYAXQK0xxnol1+vsDsMZ+TAZmADPseW4G1mH12FgGjApDnL3s9X9vx1KzTd3jFOBpe5v/CAwN0zZN+P/t3U9oHGUYx/HvzwjGeqiI4p+CFlNL8dSDUrAV4sVLe7BIKVLFqAhFUCmC3gq9ib3oQREUsuChkEo9CFqKFVqpthX/NClitSiKoqIHxUuKNI+H51nZbJOMK03XbH4fGDL78s7k3c1kn+xM5veSb/4rO9r+F68nWcB+Av4i/7p9jLxWdhj4ur5eU33vAF7v2PbROl7PAo/0YZxnyfP87eO0/d+ENwHvLHScXOJxvlHH3yRZAG7sHmc9vuA94lKPtdpb7WOzo2/fXtOIcNyHmZk1W06noczM7D9ysTAzs0YuFmZm1sjFwszMGrlYmA2YCkW8q9/jsMHiYmE2eEYBFwu7qFwsbCBJWq2MeH9NGfN+SNKV8/RdI+m9yoX6VNJI3US4V9LpioHeXn1HJR2RNCHpK0nPS9oh6WT1G6l+LUmvSvqg+m2p9mFJ49X3M0n3VPuYpAOSDirjxl/oGN+9kj6qse2vGJh2RPWeap+StK6yxHYCu5TR1XdL2lbP45Sko4v5utsAW+wbObx46ccCrCazitbX4wngwXn6ngC21voweQPf/WSK6xBwPfA9GR0/Cvxe61cAPwJ7atungRdrvQUcJP8gu4284WoYeAYYrz7rar/DwBgZMb6yHn9HRk9cCxwFrqptnqMiq8mI6idr/QnqZj264rjJm9FW1frV/f7ZeFmaiz9Z2CD7NiLaE8d8QhaQWSoyYVVEvAUQEdORsRWbgH2ReUK/AEeAO2uzjyNj5M+Rd/oeqvapru8xEREzkamh35DFYRN5NzER8SVZFNZW/8MR8UdETANfALeQc4DcDhxTToLzcLW3tXPD5nx+5RjQkvQ4WfzMenZ5vwdgtojOdayfB+Y6DTXffCULzWPSud+ZjsczzP6d6o5HiB72e772JXLOigcatmn3v0BE7JS0AdgMfC5pfUT0M/fKliB/srBlLTL6/QdJ98E/U+uuIE/9bJc0JOk6ckazkz3ufpuky+o6xq3Amdrvjvpea4Gbq30+x4GNktbUNitqu4X8Sc6wSG0zEhEnImI38Buzk1XN/hUXCzN4CHhK0iTwIXADGfk8SQa1vQ88GxE/97jfM+Tpq3fJULhp4BVgSNIUGT89Vqez5hQRv5LXM/bV+I6Tp7MW8jawtX2BG9hbF8BPk8XqVI/Pw8xBgmaLQVKLnNP7zX6Pxexi8CcLMzNr5E8WtmxIehnY2NX8UkSM92M8ZkuJi4WZmTXyaSgzM2vkYmFmZo1cLMzMrJGLhZmZNXKxMDOzRi4WZmbW6G8Jzc7NzylKJgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(list(zip(train_scores[1:20], test_scores[1:20])), linewidth=5)\n",
    "plt.title('Model score vs. number of components')\n",
    "plt.xlabel('n_components')\n",
    "plt.ylabel('Score (accuracy)')\n",
    "plt.legend(['train score', 'test score'], loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.35211773, 0.43025616, 0.47743884, 0.51540716, 0.55063205,\n",
       "       0.57980551, 0.60161763, 0.62276734, 0.64263677, 0.66177181,\n",
       "       0.67886498, 0.69551356, 0.7104849 , 0.72445063, 0.7380419 ,\n",
       "       0.75065726, 0.76254088, 0.77424256, 0.78483297, 0.79468571,\n",
       "       0.80398771, 0.81320206, 0.82155642, 0.82883374, 0.83599545,\n",
       "       0.8429198 , 0.84949993, 0.85560125, 0.86153743])"
      ]
     },
     "execution_count": 705,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_transformer.explained_variance_ratio_.cumsum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the best number of components is 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_transformer = PCA(6)\n",
    "X_train_pca = pca_transformer.fit_transform(X_train)\n",
    "X_test_pca = pca_transformer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT:\n",
      "===\n",
      "Train: 0.6457720489918168\n",
      "Test:  0.6247816321437485\n",
      "\n",
      "K-NN:\n",
      "=====\n",
      "Train: 0.6590896935337219\n",
      "Test:  0.6164212627901173\n",
      "\n",
      "LR:\n",
      "===\n",
      "Train: 0.6204203882975878\n",
      "Test:  0.6166708260544048\n",
      "\n",
      "SVM:\n",
      "====\n",
      "Train: 0.6386586083328877\n",
      "Test:  0.6303968055902172\n"
     ]
    }
   ],
   "source": [
    "print('DT:\\n===')\n",
    "DT_clf = DecisionTreeClassifier(max_depth=20, min_samples_split=100, min_samples_leaf=100)\n",
    "DT_clf.fit(X_train_pca, y_train)\n",
    "print('Train:', DT_clf.score(X_train_pca, y_train))\n",
    "print('Test: ', DT_clf.score(X_test_pca, y_test))\n",
    "\n",
    "print('\\nK-NN:\\n=====')\n",
    "KNN_clf = KNeighborsClassifier(n_neighbors=15)\n",
    "KNN_clf.fit(X_train_pca, y_train)\n",
    "print('Train:', KNN_clf.score(X_train_pca, y_train))\n",
    "print('Test: ', KNN_clf.score(X_test_pca, y_test))\n",
    "\n",
    "print('\\nLR:\\n===')\n",
    "LR_clf = LogisticRegression().fit(X_train_pca, y_train)\n",
    "print('Train:', LR_clf.score(X_train_pca, y_train))\n",
    "print('Test: ', LR_clf.score(X_test_pca, y_test))\n",
    "\n",
    "print('\\nSVM:\\n====')\n",
    "SVM_clf = SVC().fit(X_train_pca, y_train)\n",
    "print('Train:', SVM_clf.score(X_train_pca, y_train))\n",
    "print('Test: ', SVM_clf.score(X_test_pca, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Using Pipeline (with K-NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results for 1 components:  Train: 0.5882227095255923 Test: 0.5319440978287996\n",
      "results for 2 components:  Train: 0.618387976680751 Test: 0.5673820813576241\n",
      "results for 3 components:  Train: 0.6265176231480987 Test: 0.5782380833541303\n",
      "results for 4 components:  Train: 0.6365727121998181 Test: 0.5927127526828051\n",
      "results for 5 components:  Train: 0.6391934534952132 Test: 0.5907162465685051\n",
      "results for 6 components:  Train: 0.6559875915922341 Test: 0.6119291240329423\n",
      "results for 7 components:  Train: 0.6613360432154891 Test: 0.620039930122286\n",
      "results for 8 components:  Train: 0.6626196716050703 Test: 0.621537309708011\n",
      "results for 9 components:  Train: 0.6607477135369311 Test: 0.619540803593711\n",
      "results for 10 components:  Train: 0.6591966625661871 Test: 0.6184177689044172\n",
      "results for 11 components:  Train: 0.660159383858373 Test: 0.6150486648365361\n",
      "results for 12 components:  Train: 0.6583409103064662 Test: 0.6159221362615424\n",
      "results for 13 components:  Train: 0.6614964967641868 Test: 0.6181682056401298\n",
      "results for 14 components:  Train: 0.6597315077285126 Test: 0.6177938607436986\n",
      "results for 15 components:  Train: 0.6574851580467455 Test: 0.6136760668829548\n",
      "results for 16 components:  Train: 0.6577525806279082 Test: 0.6138008485150985\n",
      "results for 17 components:  Train: 0.6593571161148848 Test: 0.6146743199401048\n",
      "results for 18 components:  Train: 0.6563084986896294 Test: 0.6111804342400798\n",
      "results for 19 components:  Train: 0.6592501470824197 Test: 0.6144247566758173\n"
     ]
    }
   ],
   "source": [
    "# find best number of components\n",
    "\n",
    "for n in range (1,20):\n",
    "    steps = [('reduce_dim', PCA(n)), ('clf', neighbors.KNeighborsClassifier(n_neighbors=15))]\n",
    "    pipe = Pipeline(steps)    \n",
    "    pipe.fit(X_train, y_train)\n",
    "    print(f'results for {n} components: ', 'Train:', pipe.score(X_train, y_train), 'Test:', pipe.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best result with number of components = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results for 5 n_neighbors:  Train: 0.6883457239129273 Test: 0.5975792363364113\n",
      "results for 6 n_neighbors:  Train: 0.6850296839065091 Test: 0.6086848015972048\n",
      "results for 7 n_neighbors:  Train: 0.6794138097020912 Test: 0.6144247566758173\n",
      "results for 8 n_neighbors:  Train: 0.6770070064716265 Test: 0.6126778138258049\n",
      "results for 9 n_neighbors:  Train: 0.6752420174359522 Test: 0.6151734464686798\n",
      "results for 10 n_neighbors:  Train: 0.6712306787185109 Test: 0.6157973546293986\n",
      "results for 11 n_neighbors:  Train: 0.6695191741990694 Test: 0.6209134015472922\n",
      "results for 12 n_neighbors:  Train: 0.67016098839386 Test: 0.6186673321687047\n",
      "results for 13 n_neighbors:  Train: 0.668021607744558 Test: 0.6162964811579735\n",
      "results for 14 n_neighbors:  Train: 0.6650799593517677 Test: 0.6254055403044672\n",
      "results for 15 n_neighbors:  Train: 0.6626731561213028 Test: 0.6270277015223359\n",
      "results for 16 n_neighbors:  Train: 0.6618708883778146 Test: 0.6219116546044422\n",
      "results for 17 n_neighbors:  Train: 0.6609081670856287 Test: 0.6211629648115797\n",
      "results for 18 n_neighbors:  Train: 0.6607477135369311 Test: 0.6244072872473172\n",
      "results for 19 n_neighbors:  Train: 0.6589292399850243 Test: 0.6236585974544547\n"
     ]
    }
   ],
   "source": [
    "# find best number of n_neighbors\n",
    "\n",
    "for k in range (5,20):\n",
    "    steps = [('reduce_dim', PCA(8)), ('clf', neighbors.KNeighborsClassifier(n_neighbors=k))]\n",
    "    pipe = Pipeline(steps)    \n",
    "    pipe.fit(X_train, y_train)\n",
    "    print(f'results for {k} n_neighbors: ', 'Train:', pipe.score(X_train, y_train), 'Test:', pipe.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best result with number of n_neighbors = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Using Cross Validation (with SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 725,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C = 1: mean=0.6155523032641212 std=0.00781287517387333\n",
      "C = 51: mean=0.6364118396030005 std=0.01174148416356297\n",
      "C = 101: mean=0.6371610032588644 std=0.011184285090547207\n",
      "C = 151: mean=0.6373209878203334 std=0.01241519720550821\n",
      "C = 201: mean=0.6379091088038396 std=0.012556083923854752\n",
      "C = 251: mean=0.6380699513599974 std=0.011803805746531849\n",
      "C = 301: mean=0.6384976730869051 std=0.010878269501982663\n",
      "C = 351: mean=0.6388722764178443 std=0.010713713057891263\n",
      "C = 401: mean=0.6386581867764296 std=0.010527612552631634\n",
      "C = 451: mean=0.6382840410931419 std=0.01082130414070127\n"
     ]
    }
   ],
   "source": [
    "for c in range (1,501,50):\n",
    "    steps = [('reduce_dim', PCA(8)), ('clf', SVC(C=c))]\n",
    "    pipe = Pipeline(steps)    \n",
    "    scores = cross_val_score(pipe, X_train, y_train, cv=5)\n",
    "    print(f'C = {c}: mean={scores.mean()} std={scores.std()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.6571107664331176\n",
      "Test: 0.6385076116795607\n"
     ]
    }
   ],
   "source": [
    "steps = [('reduce_dim', PCA(8)), ('clf', SVC(C=350))]\n",
    "pipe = Pipeline(steps) \n",
    "pipe.fit(X_train, y_train)\n",
    "print('Train:', pipe.score(X_train, y_train))\n",
    "print('Test:', pipe.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Grid Search (with DT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 752,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-752-d81081a57b71>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-752-d81081a57b71>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    stop here <<<>>>\u001b[0m\n\u001b[1;37m            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "stop here <<<>>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 734,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6194041824891694\n",
      "{'clf__max_depth': 7, 'clf__min_samples_leaf': 50, 'clf__min_samples_split': 70, 'reduce_dim__n_components': 8}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6184177689044172"
      ]
     },
     "execution_count": 734,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps = [('reduce_dim', PCA()), ('clf', DecisionTreeClassifier())]\n",
    "pipe = Pipeline(steps) \n",
    "\n",
    "parameters = {\n",
    "    'reduce_dim__n_components': range (5, 10),\n",
    "    'clf__max_depth': range(2, 10),\n",
    "    'clf__min_samples_split': [30, 50, 70, 100],\n",
    "    'clf__min_samples_leaf': range(10, 110, 10)}\n",
    "\n",
    "grid_search = GridSearchCV(pipe, parameters, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(grid_search.best_score_)\n",
    "print(grid_search.best_params_)\n",
    "grid_search.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using these \"best parameters\" with the \"naive\" model as shown at the top of the page, will provide better results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 748,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.6497833877092581\n",
      "Test:  0.6425006239081608\n"
     ]
    }
   ],
   "source": [
    "X = outcomes2.drop('OutcomeType', axis=1)\n",
    "y = outcomes2.OutcomeType\n",
    "X = pd.get_dummies(X)\n",
    "X_train, X_test, y_train, y_test = split(X, y, train_size=0.7, random_state=4014)\n",
    "DT_clf = DecisionTreeClassifier(max_depth=7, min_samples_split=70, min_samples_leaf=50)\n",
    "DT_clf.fit(X_train, y_train)\n",
    "print('Train:', DT_clf.score(X_train, y_train))\n",
    "print('Test: ', DT_clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Best score so far...**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Using Ensemble methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.1. Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 754,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = outcomes2.drop('OutcomeType', axis=1)\n",
    "y = outcomes2.OutcomeType\n",
    "cols = [col for col in list(X.columns) if col != 'Named']\n",
    "X = pd.get_dummies(X, columns=cols)\n",
    "X_train, X_test, y_train, y_test = split(X, y, train_size=0.7, random_state=4014)\n",
    "\n",
    "clf1 = KNeighborsClassifier(n_neighbors=8)\n",
    "clf2 = DecisionTreeClassifier(max_depth=7, min_samples_split=70, min_samples_leaf=50)\n",
    "clf3 = SVC(350)\n",
    "clf4 = LogisticRegression()\n",
    "\n",
    "classifiers = [('KNN', clf1), ('DT', clf2), ('SVM', clf3), ('LR', clf4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 755,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('KNN', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=8, p=2,\n",
       "           weights='uniform')), ('DT', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=7,\n",
       "            max_features=None, max...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))],\n",
       "         flatten_transform=None, n_jobs=1, voting='hard', weights=None)"
      ]
     },
     "execution_count": 755,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_voting = VotingClassifier(estimators=classifiers, voting='hard')\n",
    "clf_voting.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 757,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting classifier:\n",
      "     \ttrain accuracy: 0.6692\n",
      "     \ttest accuracy: 0.6451\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "       Adoption       0.62      0.89      0.73      3273\n",
      "           Died       0.00      0.00      0.00        67\n",
      "     Euthanasia       0.58      0.08      0.14       455\n",
      "Return_to_owner       0.50      0.39      0.44      1425\n",
      "       Transfer       0.77      0.60      0.67      2794\n",
      "\n",
      "    avg / total       0.64      0.65      0.62      8014\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"{:3} classifier:\\n \\\n",
    "    \\ttrain accuracy: {:.4f}\\n \\\n",
    "    \\ttest accuracy: {:.4f}\"\\\n",
    "    .format('Voting', \n",
    "            clf_voting.score(X_train, y_train), \n",
    "            clf_voting.score(X_test, y_test)))\n",
    "print(classification_report(y_test, clf_voting.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Best score so far! :) **\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 758,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting classifier:\n",
      "     \ttrain accuracy: 0.6702\n",
      "     \ttest accuracy: 0.6442\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "       Adoption       0.62      0.88      0.73      3273\n",
      "           Died       0.00      0.00      0.00        67\n",
      "     Euthanasia       0.59      0.08      0.14       455\n",
      "Return_to_owner       0.50      0.39      0.44      1425\n",
      "       Transfer       0.77      0.60      0.67      2794\n",
      "\n",
      "    avg / total       0.64      0.64      0.62      8014\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run the above Voting on the original \"naive\" data\n",
    "\n",
    "X = outcomes2.drop('OutcomeType', axis=1)\n",
    "y = outcomes2.OutcomeType\n",
    "X = pd.get_dummies(X)\n",
    "X_train, X_test, y_train, y_test = split(X, y, train_size=0.7, random_state=4014)\n",
    "clf1 = KNeighborsClassifier(n_neighbors=8)\n",
    "clf2 = DecisionTreeClassifier(max_depth=7, min_samples_split=70, min_samples_leaf=50)\n",
    "clf3 = SVC(350)\n",
    "clf4 = LogisticRegression()\n",
    "classifiers = [('KNN', clf1), ('DT', clf2), ('SVM', clf3), ('LR', clf4)]\n",
    "clf_voting = VotingClassifier(estimators=classifiers, voting='hard')\n",
    "clf_voting.fit(X_train, y_train)\n",
    "print(\"{:3} classifier:\\n \\\n",
    "    \\ttrain accuracy: {:.4f}\\n \\\n",
    "    \\ttest accuracy: {:.4f}\"\\\n",
    "    .format('Voting', \n",
    "            clf_voting.score(X_train, y_train), \n",
    "            clf_voting.score(X_test, y_test)))\n",
    "print(classification_report(y_test, clf_voting.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bit worse than the earlier one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.2 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 762,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = outcomes2.drop('OutcomeType', axis=1)\n",
    "y = outcomes2.OutcomeType\n",
    "cols = [col for col in list(X.columns) if col != 'Named']\n",
    "X = pd.get_dummies(X, columns=cols)\n",
    "X_train, X_test, y_train, y_test = split(X, y, train_size=0.7, random_state=4014)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 793,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.8142482751243515\n",
      "Test:  0.5929623159470926\n"
     ]
    }
   ],
   "source": [
    "# using default parameters\n",
    "\n",
    "RF_clf = RandomForestClassifier(random_state=4014) \n",
    "RF_clf.fit(X_train, y_train)\n",
    "\n",
    "print('Train:', RF_clf.score(X_train, y_train))\n",
    "print('Test: ', RF_clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 794,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.6155532973204257\n",
      "Test:  0.6128025954579486\n"
     ]
    }
   ],
   "source": [
    "# using the DT parameters\n",
    "\n",
    "RF_clf = RandomForestClassifier(max_depth=7, min_samples_split=70, min_samples_leaf=50, random_state=4014) \n",
    "RF_clf.fit(X_train, y_train)\n",
    "\n",
    "print('Train:', RF_clf.score(X_train, y_train))\n",
    "print('Test: ', RF_clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 795,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.6772209445365567\n",
      "Test:  0.6381332667831295\n"
     ]
    }
   ],
   "source": [
    "# using other parameters (no fear of overfit)\n",
    "\n",
    "RF_clf = RandomForestClassifier(max_depth=15, min_samples_split=2, min_samples_leaf=2, random_state=4014) \n",
    "RF_clf.fit(X_train, y_train)\n",
    "\n",
    "print('Train:', RF_clf.score(X_train, y_train))\n",
    "print('Test: ', RF_clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.3. AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 796,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT ADA boosting classifier:\n",
      "     \ttrain accuracy: 0.63\n",
      "     \ttest accuracy: 0.63\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "       Adoption       0.59      0.94      0.72      3273\n",
      "           Died       0.00      0.00      0.00        67\n",
      "     Euthanasia       0.00      0.00      0.00       455\n",
      "Return_to_owner       0.48      0.20      0.28      1425\n",
      "       Transfer       0.76      0.59      0.67      2794\n",
      "\n",
      "    avg / total       0.59      0.63      0.58      8014\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = outcomes2.drop('OutcomeType', axis=1)\n",
    "y = outcomes2.OutcomeType\n",
    "cols = [col for col in list(X.columns) if col != 'Named']\n",
    "X = pd.get_dummies(X, columns=cols)\n",
    "X_train, X_test, y_train, y_test = split(X, y, train_size=0.7, random_state=4014)\n",
    "\n",
    "clf_base = DecisionTreeClassifier(max_depth=3)\n",
    "clf_adaboost = AdaBoostClassifier(base_estimator=clf_base, n_estimators=200, learning_rate=0.01)\n",
    "clf_adaboost.fit(X_train, y_train)\n",
    "\n",
    "print(\"{:3} classifier:\\n \\\n",
    "    \\ttrain accuracy: {:.4f}\\n \\\n",
    "    \\ttest accuracy: {:.4f}\"\\\n",
    "    .format('DT ADA boosting', \n",
    "            clf_adaboost.score(X_train, y_train), \n",
    "            clf_adaboost.score(X_test, y_test)))\n",
    "print(classification_report(y_test, clf_adaboost.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.4. Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 797,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT gradient boosting classifier:\n",
      "     \ttrain accuracy: 0.6433\n",
      "     \ttest accuracy: 0.6373\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "       Adoption       0.64      0.83      0.73      3273\n",
      "           Died       0.00      0.00      0.00        67\n",
      "     Euthanasia       0.66      0.05      0.10       455\n",
      "Return_to_owner       0.47      0.43      0.45      1425\n",
      "       Transfer       0.72      0.63      0.67      2794\n",
      "\n",
      "    avg / total       0.63      0.64      0.61      8014\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = outcomes2.drop('OutcomeType', axis=1)\n",
    "y = outcomes2.OutcomeType\n",
    "cols = [col for col in list(X.columns) if col != 'Named']\n",
    "X = pd.get_dummies(X, columns=cols)\n",
    "X_train, X_test, y_train, y_test = split(X, y, train_size=0.7, random_state=4014)\n",
    "\n",
    "clf_GB = GradientBoostingClassifier(max_depth=3, n_estimators=200, learning_rate=0.01)\n",
    "clf_GB.fit(X_train, y_train)\n",
    "\n",
    "print(\"{:3} classifier:\\n \\\n",
    "    \\ttrain accuracy: {:.4f}\\n \\\n",
    "    \\ttest accuracy: {:.4f}\"\\\n",
    "    .format('DT gradient boosting', \n",
    "            clf_GB.score(X_train, y_train), \n",
    "            clf_GB.score(X_test, y_test)))\n",
    "print(classification_report(y_test, clf_GB.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Along the way, I've tried to manipulate the data in several ways, especially regarding the features that I've added to the dataset in part1 of the project (dogs features):\n",
    "\n",
    "- Leaving these features as-is (with values 1-5 and 999 for the missing data)\n",
    "- Converting the 999 values to median values of each feature\n",
    "- Filtering out the 999 values\n",
    "- Categorizing these features\n",
    "\n",
    "I also tried to apply Scaling were I thought that might improve the result.\n",
    "\n",
    "In addition, I applied all the classification models that we've learned in the course, trying to find the best hyperparameters for each model.\n",
    "\n",
    "The best score I've reached - while using the Voting method - was **0.6451** (as shown above, applying the model on the Cats portion of the dataset provided us with a higher score)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
